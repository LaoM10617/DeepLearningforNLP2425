{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f506b01-f3ba-4744-9953-6a8d30f24717",
   "metadata": {
    "id": "9f506b01-f3ba-4744-9953-6a8d30f24717"
   },
   "source": [
    "# Deep Learning for NLP - Exercise 03\n",
    "In this exercise, we will be using the Hugging Face ecosystem to finetune BERT for a classification task (part 1) and T5 for text summarization (part 2). Then, we will upload our training and validation results via TensorBoard, which makes interactive sharing and inspecting of results very easy.\n",
    "\n",
    "Part 1 and part 2 can be worked on independently.\n",
    "\n",
    "___\n",
    "General hints:\n",
    "* Have a look at the imports below when solving the tasks\n",
    "* Use the given modules and all submodules of the imports, but don't import anything else!\n",
    "    * For instance, you can use other functions under the `torch` or `nn` namespace, but don't import e.g. PyTorch Lightning, etc.\n",
    "* It is recommended to install all packages from the provided environment file\n",
    "* Feel free to test your code between sub-tasks of the exercise sheet, so that you can spot mistakes early (wrong shapes, impossible numbers, NaNs, ...)\n",
    "* Just keep in mind that your final submission should be compliant to the provided initial format of this file\n",
    "\n",
    "Submission guidelines:\n",
    "* Make sure that the code runs on package versions from the the provided environment file\n",
    "* Do not add or change any imports (also don't change the naming of imports, e.g. `torch.nn.functional as f`)\n",
    "* Remove your personal, additional code testings and experiments throughout the notebook\n",
    "* Do not change the class, function or naming structure as we will run tests on the given names\n",
    "* Additionally export this notebook as a `.py` file, and submit **both** the executed `.ipynb` notebook with plots in it **and** the `.py` file\n",
    "* **Deviation from the above guidelines will result in partial or full loss of points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa3cf98-b17b-4c72-b876-c1f1d67b1486",
   "metadata": {
    "id": "eaa3cf98-b17b-4c72-b876-c1f1d67b1486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.24.0\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.24.0) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.24.0) (0.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.24.0) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.24.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.24.0) (4.66.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.24.0) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.24.0) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (2023.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\13020\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers==4.24.0) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.24.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.24.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.24.0) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.24.0) (3.3)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.0\n",
      "    Uninstalling transformers-4.40.0:\n",
      "      Successfully uninstalled transformers-4.40.0\n",
      "Successfully installed transformers-4.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 2.7.0 requires transformers<5.0.0,>=4.34.0, but you have transformers 4.24.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.24.0\n",
    "# !pip install datasets==3.0.1\n",
    "# !pip install evaluate==0.4.0\n",
    "# !pip install rouge-score==0.1.2\n",
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040d626-232c-4f9a-be5e-aaad77ecfbb0",
   "metadata": {
    "id": "a040d626-232c-4f9a-be5e-aaad77ecfbb0",
    "tags": []
   },
   "source": [
    "# Task 1: Sequence Classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbdc10-854d-4c00-97b1-cedcd0bdde77",
   "metadata": {
    "id": "12fbdc10-854d-4c00-97b1-cedcd0bdde77"
   },
   "source": [
    "* In this task, we will finetune [BERT](https://huggingface.co/bert-base-uncased) on a custom sentiment classification dataset and perform multi-class sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5529e86-d1c1-4414-b0d3-56d6931f21e5",
   "metadata": {
    "id": "d5529e86-d1c1-4414-b0d3-56d6931f21e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67aefa49-05b1-4b82-ba9d-ad0ebf4742f5",
   "metadata": {
    "id": "67aefa49-05b1-4b82-ba9d-ad0ebf4742f5"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb73ae-5cba-4c54-a65d-7af604c22fd9",
   "metadata": {
    "id": "1cdb73ae-5cba-4c54-a65d-7af604c22fd9"
   },
   "source": [
    "We start by downloading the [DAIR-AI Emotion Dataset](https://huggingface.co/datasets/dair-ai/emotion) from the Hugging Face Hub, which already comes with a train, validation, and test split of 16,000 - 2,000 - 2,000 samples. The dataset consists of tweets with their labeled emotions over 6 different classes.\n",
    "\n",
    "* As the dataset, by default, consists of integer labels, we first create two helper dictionaries\n",
    "    * `idx2lbl`, which maps the integer index to the label string\n",
    "    * `lbl2idx`, which maps the label strings to the integer index\n",
    "    * The downloaded Hugging Face dataset object implements a useful `.features` method for each dataset split, which contains the label strings in the correct order corresponding to the integers (the labels are consistent across each split, so it is enough to only inspect one split)\n",
    "* Try to get an overview of the data and the dataset format by printing 10 random (use the `random` library to sample random numbers!) samples of the train split with their corresponding integer and string labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90261bf4-0b3f-44ae-af4c-2d2ed2d58d36",
   "metadata": {
    "id": "90261bf4-0b3f-44ae-af4c-2d2ed2d58d36"
   },
   "outputs": [],
   "source": [
    "emotion_dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb96f49b-f101-47cb-8d69-febe03928969",
   "metadata": {
    "id": "cb96f49b-f101-47cb-8d69-febe03928969"
   },
   "outputs": [],
   "source": [
    "label_features = emotion_dataset['train'].features['label']\n",
    "idx2lbl = {i: label for i, label in enumerate(label_features.names)}\n",
    "lbl2idx = {label: i for i, label in enumerate(label_features.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a160b43-c6ab-47fb-a727-907ec2a4c04a",
   "metadata": {
    "id": "4a160b43-c6ab-47fb-a727-907ec2a4c04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: i did sleep last night however but woke up at am feeling splendid other than sniffles and itchy throat and just wasnt sure how i could be so awake, Label: joy\n",
      "Text: out on a weekend with a group of people, Label: anger\n",
      "Text: i feel rejected by someone then what part of myself am i rejecting, Label: sadness\n",
      "Text: i feel like if i had a job worth caring about i wouldn t be so shifty, Label: love\n",
      "Text: im feeling dull and bored, Label: sadness\n",
      "Text: i couldn t feel positive emotions of any sort, Label: joy\n",
      "Text: i feel quite lucky to have stumbled upon it, Label: joy\n",
      "Text: i asked feeling hesitant, Label: fear\n",
      "Text: i feel fantastic and i find that i have a renewed sense of strength and endurance, Label: joy\n",
      "Text: on a dark night i felt that there were several people near me and i did not know who they were, Label: fear\n"
     ]
    }
   ],
   "source": [
    "rand_idxs = random.sample(range(len(emotion_dataset['train'])), 10)\n",
    "for idx in rand_idxs:\n",
    "    example = emotion_dataset['train'][idx]\n",
    "    print(f\"Text: {example['text']}, Label: {idx2lbl[example['label']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd221019-21e4-4dcc-9cdb-fb37d10140ab",
   "metadata": {
    "id": "cd221019-21e4-4dcc-9cdb-fb37d10140ab"
   },
   "source": [
    "To start the preprocessing steps, we first need to load the pre-trained tokenizer for our [bert-base-uncased](https://huggingface.co/bert-base-uncased) model, which can be achieved very comfortable using the `AutoTokenizer` [class API](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d58ef2-7f79-4985-8dc1-2f8743c96e80",
   "metadata": {
    "id": "46d58ef2-7f79-4985-8dc1-2f8743c96e80"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074cb17-ec06-4ea5-8050-37686c6e0ee4",
   "metadata": {
    "id": "3074cb17-ec06-4ea5-8050-37686c6e0ee4"
   },
   "source": [
    "If we enter an example input from the train set into the tokenizer, we can see that we receive again dictionaries as output, which is the preferred data format from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f946f992-2676-49dc-b6a2-c0ecc9e45502",
   "metadata": {
    "id": "f946f992-2676-49dc-b6a2-c0ecc9e45502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: i didnt feel humiliated\n",
      "Tokenized Output: {'input_ids': tensor([[  101,  1045,  2134,  2102,  2514, 26608,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "example_text = emotion_dataset['train'][0]['text']\n",
    "example_seq = tokenizer(example_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Original Text:\", example_text)\n",
    "print(\"Tokenized Output:\", example_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159455f-03c7-4fe0-ae53-36ab44aafa65",
   "metadata": {
    "id": "3159455f-03c7-4fe0-ae53-36ab44aafa65"
   },
   "source": [
    "* We can see that the dictionary contains `input_ids`, which represent the tokenized sequence *including* specials tokens\n",
    "    * For instance, notice that the sequence begins with `101`, which is used by BERT to mark the `[CLS]` token\n",
    "    * The sequence ends with the `102` token, which BERT uses as a separation mark between sequences, `[SEP]`\n",
    "    * You can check all special tokens using `tokenizer.all_special_tokens` and `tokenizer.all_special_ids`\n",
    "* Secondly, the tokenizer contains `token_type_ids`, which are used to distinguish different segments of input tokens in models that support token-level type embeddings, like BERT and its variants\n",
    "    * These embeddings are useful for tasks like sentence pair classification or question answering, where the model needs to understand which tokens belong to which part of the input. (not needed here)\n",
    "* Thirdly, the tokenizer automatically creates the attention mask, which consists of all `1` in this case\n",
    "    * The attention mask in input sequences is usually only `0` when padding symbols are added, as we don't want to attend to padding symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af8005-8c7c-4dce-b34a-074576892e81",
   "metadata": {
    "id": "02af8005-8c7c-4dce-b34a-074576892e81"
   },
   "source": [
    "Following this, we can use the tokenizer to tokenize our entire dataset.\n",
    "\n",
    "* Hugging Face dataset objects provide a way to `map` a function onto every single object using powerful parallel processing operations\n",
    "* There, we need to create a function which acts like it:\n",
    "    * takes in text examples of a data split of our dataset object\n",
    "    * inputs it into the tokenizer\n",
    "        * use the tokenizer with options `truncation=True` and `max_length=512`, since the maximum context size of BERT is 512 tokens\n",
    "        * This shouldn't impact our dataset in any way since the Tweet dataset stems from the times when tweets were limited to 280 characters, but it's better to be safe than sorry and have your training interrupted\n",
    "    * return the output of the tokenizer\n",
    "* Apply the function onto the entire dataset object using its `map` method with the option `batched=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827f4441-849c-4e8c-b4cf-07721fd1e823",
   "metadata": {
    "id": "827f4441-849c-4e8c-b4cf-07721fd1e823"
   },
   "outputs": [],
   "source": [
    "def tokenize_seqs(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = emotion_dataset.map(tokenize_seqs, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e1605-da0d-4dff-868b-720ae98f06a1",
   "metadata": {
    "id": "0e4e1605-da0d-4dff-868b-720ae98f06a1"
   },
   "source": [
    "* Lastly, the [Hugging Face Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) expects the class labels to be named `labels`, so we need to rename the label column from `label` to `labels`\n",
    "    * Make use of the provided functions in the [Hugging Face Datasets API](https://huggingface.co/docs/datasets/process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488618a4-3598-465f-b4d7-7454cacad50f",
   "metadata": {
    "id": "488618a4-3598-465f-b4d7-7454cacad50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i didnt feel humiliated', 'labels': 0, 'input_ids': [101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "#print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d3411-8b9d-44da-9d80-f5347ec247ea",
   "metadata": {
    "id": "972d3411-8b9d-44da-9d80-f5347ec247ea"
   },
   "source": [
    "* We can now load our pre-trained BERT model [bert-base-uncased](https://huggingface.co/bert-base-uncased) from the Hugging Face Hub\n",
    "    * Use the `AutoModelForSequenceClassification`, see [here](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)\n",
    "    * Specifically, use the `from_pretrained` [method](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification.from_pretrained) and provide the number of labels, and the `id2lbl` and `lblidx` functions we created earlier\n",
    "    * This will configure a `PretrainedConfig` object, which will behave differently depending on whether we train or fine-tune. Have a look [here](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/configuration#transformers.PretrainedConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3cb65e0-a4fa-470a-a636-fdd5ea1bf405",
   "metadata": {
    "id": "a3cb65e0-a4fa-470a-a636-fdd5ea1bf405"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"sadness\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"love\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"fear\",\n",
      "    \"5\": \"surprise\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"fear\": 4,\n",
      "    \"joy\": 1,\n",
      "    \"love\": 2,\n",
      "    \"sadness\": 0,\n",
      "    \"surprise\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(idx2lbl)  \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=idx2lbl,\n",
    "    label2id=lbl2idx\n",
    ")\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919626c-23fd-40e7-83c3-35d2f6ee894d",
   "metadata": {
    "id": "8919626c-23fd-40e7-83c3-35d2f6ee894d"
   },
   "source": [
    "Next, we are going to define a function that calculates our desired training objective, in our case, the F1 score. In typical Hugging Face fashion, it needs to return a dictionary\n",
    "* First, the function will be called `compute_metrics`\n",
    "    * In theory, it could be any name, but should include *all* metrics you want the return from training, each with its key-value pair in the dictionary\n",
    "    * The key should always represent the name of the metric, and the value should be the calculated metric  \n",
    "* Secondly, it will take a `eval_preds` parameter\n",
    "    * This is an intermediate result type output from the Hugging Face `Trainer` object\n",
    "    * It is a `NamedTuple` of type [EvalPred](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/configuration#transformers.PretrainedConfig), which holds our predictions and label ids\n",
    "    * You can access them using the `eval_preds.label_ids`, and `eval_preds.predictions` methods\n",
    "* Extract the predictions and labels\n",
    "* Find the predicted class index using `argmax()` with the correct dimension parameter\n",
    "* Calculate the F1 score using the previously imported sklearn function\n",
    "    * Since we have a multi-class classification problem, use the `average='weighted'` choice\n",
    "    * Other settings can also make sense, but in this case, we use the `'weighted'` option\n",
    "* Return a dictionary in the style `{'f1': f1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6590d17d-a3c9-4721-b4c4-8313c530d529",
   "metadata": {
    "id": "6590d17d-a3c9-4721-b4c4-8313c530d529"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    preds = logits.argmax(axis=-1) \n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340e5ff-a53c-4e0f-a4bc-958eae5f28eb",
   "metadata": {
    "id": "f340e5ff-a53c-4e0f-a4bc-958eae5f28eb"
   },
   "source": [
    "Finally, we are putting it all together in a [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) instance. It has *a lot* of options, so go and have a look at the above link to the documentation.\n",
    "\n",
    "In our case, we will set the following:\n",
    "* [output_dir](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.output_dir), the directory where you want to save model checkpoints and logging files\n",
    "    * Use something like `'./logs/run1'`, because we will later run more experiments, which should then be named `'./logs/run2/'`, so that everything is contained in one directory\n",
    "* [per_device_train_batch_size](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size), the batch size used for training\n",
    "    * since we need to store all ~110mio. parameters of BERT, the gradient for all parameters, and one tokenized batch on the GPU, this will be a rather small batch size of e.g. 4 or 8 (see below for some tricks we can use to artifically increase the effective training batch size)\n",
    "* [per_device_eval_batch_size](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.per_device_eval_batch_size), the batch size used for evaluation\n",
    "    * can be approximately 4x the training batch size\n",
    "* [gradient_accumulation_steps](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps), adds up the gradients of multiple batches in order to artificially increase the batch size while keeping GPU memory lower\n",
    "    * set it to 2, which approximates training with double our train batch size\n",
    "    * see [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance) and [here](https://huggingface.co/docs/transformers/perf_train_gpu_one) for more memory and training speed tricks\n",
    "* [learning_rate](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.learning_rate), the learning rate for [AdamW](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/optimizer_schedules#transformers.AdamW) optimizier, a popular choice for pure transformer model training and fine-tuning\n",
    "    * set it to `2e-5`, transformers generally use much lower training rates than LSTMs or CNNs\n",
    "* [weight_decay](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.weight_decay), weight decay rate used for regularization\n",
    "    * set it to `1e-3`\n",
    "* [num_train_epochs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.num_train_epochs(float,), the number of epochs to train\n",
    "    * set it to 3\n",
    "* [evaluation_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.evaluation_strategy), in which intervals evaluation on the dev set should be performed\n",
    "    * set to `'epoch'`\n",
    "* [logging_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.logging_strategy), at which point during training logging should take place, e.g. at every $N$ steps, or per each epoch\n",
    "    * set it to `'steps'`, which requires us to set the `logging_steps` parameter\n",
    "* [logging_steps](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.logging_steps), after how many optimizer steps we should log the loss\n",
    "    * set it to `len(emotion_dataset['train']) / per_device_train_batch_size`\n",
    "* [save_strategy](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_strategy), when a model checkpoint should be save\n",
    "    * set it to `'epoch'`\n",
    "* [save_total_limit](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit), how many checkpoints should be saved\n",
    "    * set it to 1 to save save disk sapce\n",
    "* [seed](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.seed), the random seed for model initialization, which is important for experimental reproducibilty\n",
    "    * set it to 42 (it already is, by default, but just to be specific)\n",
    "* [data_seed](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.data_seed), the random seed for data loading, also important for experimental reproducibility\n",
    "    * set it also to 42\n",
    "* [fp16](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fp16), whether 16bit floating points should be used for training. This is another trick so save memory (roughly half of memory is used, because the default is 32bit floating points, see the two links above)\n",
    "    * set it to `True`\n",
    "* [dataloader_num_workers](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.dataloader_num_workers), how many workers to use for dataloading\n",
    "    * set it to 2\n",
    "* [load_best_model_at_end](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.load_best_model_at_end), allows us to automatically keep the best model (according to loss) in addition to the last saved, and load it after training for further prediction on the test set\n",
    "    * Set it to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0f100e9-7558-44c5-94ad-f272c11b6ba0",
   "metadata": {
    "id": "f0f100e9-7558-44c5-94ad-f272c11b6ba0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./logs/run1/',  \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,  \n",
    "    gradient_accumulation_steps=2,  \n",
    "    learning_rate=2e-5,  \n",
    "    weight_decay=1e-3,  \n",
    "    num_train_epochs=3,  \n",
    "    evaluation_strategy='epoch',  \n",
    "    logging_strategy='steps',  \n",
    "    logging_steps=len(tokenized_datasets['train']) // 8,  \n",
    "    save_strategy='epoch', \n",
    "    save_total_limit=1,  \n",
    "    seed=42,  \n",
    "    data_seed=42,  \n",
    "    fp16=True,  \n",
    "    dataloader_num_workers=2,  \n",
    "    load_best_model_at_end=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7807c-a6f7-44bc-b72e-235d4f815445",
   "metadata": {
    "id": "a0d7807c-a6f7-44bc-b72e-235d4f815445"
   },
   "source": [
    "* Then, we just instantiate the `Trainer`, feed all arguments (`model`, `training_args`, `compute_metrics`, `train_dataset`, `eval_dataset`, `tokenizer`) into [it](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer), and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b14524e-4acc-4a0a-b82e-eea3dede7b41",
   "metadata": {
    "id": "0b14524e-4acc-4a0a-b82e-eea3dede7b41"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  \n",
    "    args=training_args,  \n",
    "    train_dataset=tokenized_datasets[\"train\"],  \n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  \n",
    "    tokenizer=tokenizer, \n",
    "    compute_metrics=compute_metrics  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9ee3363-5c94-4be3-ae45-883a557c0529",
   "metadata": {
    "id": "d9ee3363-5c94-4be3-ae45-883a557c0529"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 109486854\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 13:31:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214456</td>\n",
       "      <td>0.929971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.159212</td>\n",
       "      <td>0.938308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.940167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./logs/run1/checkpoint-1000\n",
      "Configuration saved in ./logs/run1/checkpoint-1000\\config.json\n",
      "Model weights saved in ./logs/run1/checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run1/checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run1/checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./logs/run1/checkpoint-2000\n",
      "Configuration saved in ./logs/run1/checkpoint-2000\\config.json\n",
      "Model weights saved in ./logs/run1/checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run1/checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run1/checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./logs/run1/checkpoint-3000\n",
      "Configuration saved in ./logs/run1/checkpoint-3000\\config.json\n",
      "Model weights saved in ./logs/run1/checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run1/checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run1/checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [logs\\run1\\checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./logs/run1/checkpoint-2000 (score: 0.1592121720314026).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:2024: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
      "Deleting older checkpoint [logs\\run1\\checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [logs\\run1\\checkpoint-3000] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.23096639506022135, metrics={'train_runtime': 48702.2196, 'train_samples_per_second': 0.986, 'train_steps_per_second': 0.062, 'total_flos': 1.2629784231936e+16, 'train_loss': 0.23096639506022135, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe076d-0f0f-4a31-809c-eb370b6144fa",
   "metadata": {
    "id": "b3fe076d-0f0f-4a31-809c-eb370b6144fa"
   },
   "source": [
    "* As we specified in the `Trainer` class, we have already loaded the best model again (independent of whether it was last epoch's model or not).\n",
    "* We can now run one final epoch on the test set using `trainer.predict()`, which [returns](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.predict) us a `PredictionOutput` containing, among others, the test F1 score\n",
    "* Print the test F1 score and comment on the test set performance compared to the intermediate training evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dabb9700-ba82-40e0-a957-699c8c9bbbf0",
   "metadata": {
    "id": "dabb9700-ba82-40e0-a957-699c8c9bbbf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.9270599185507037\n"
     ]
    }
   ],
   "source": [
    "test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "print(\"Test F1 Score:\", test_predictions.metrics[\"test_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8be22-87d8-41ab-a7a5-130fcebb5ca1",
   "metadata": {
    "id": "6bb8be22-87d8-41ab-a7a5-130fcebb5ca1"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "The F1 score (0.9270) on the test set shows that the model performs very well on the target task (multi-classification problem).\n",
    "This shows that the model is able to distinguish between different sentiment categories very well.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617e14-5bf9-439b-b880-98fb430ca5f1",
   "metadata": {
    "id": "bf617e14-5bf9-439b-b880-98fb430ca5f1"
   },
   "source": [
    "However, we can go one step further in analyzing the model performances.\n",
    "\n",
    "* Start by creating a plot which shows the percentages of correct and incorrect class predictions per label\n",
    "* For example, you could extract the predicted classes and labels from the test predictions object, and plot a stacked bar chart with a sum that always equals 100%, and each part of the stack is made up of the percentages of correct and incorrect labels\n",
    "* You are also free to use other visualization techniques that show the same kind of information as long as it is clearly visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8241746-06f9-4601-b045-10965cbaf53c",
   "metadata": {
    "id": "b8241746-06f9-4601-b045-10965cbaf53c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = np.argmax(test_predictions.predictions, axis=1)  \n",
    "true_labels = test_predictions.label_ids \n",
    "\n",
    "num_classes = len(np.unique(true_labels))\n",
    "correct_counts = np.zeros(num_classes)\n",
    "incorrect_counts = np.zeros(num_classes)\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "    if predictions[i] == true_labels[i]:\n",
    "        correct_counts[true_labels[i]] += 1\n",
    "    else:\n",
    "        incorrect_counts[true_labels[i]] += 1\n",
    "\n",
    "total_counts = correct_counts + incorrect_counts\n",
    "correct_percentages = correct_counts / total_counts * 100\n",
    "incorrect_percentages = incorrect_counts / total_counts * 100\n",
    "\n",
    "labels = [f\"Class {i}\" for i in range(num_classes)]\n",
    "x = np.arange(num_classes)\n",
    "\n",
    "plt.bar(x, correct_percentages, label=\"Correct\", color=\"green\")\n",
    "plt.bar(x, incorrect_percentages, bottom=correct_percentages, label=\"Incorrect\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Correct vs Incorrect Predictions per Class\")\n",
    "plt.xticks(x, labels)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511b6f7-4370-43f6-a8fd-5c60b3331881",
   "metadata": {
    "id": "2511b6f7-4370-43f6-a8fd-5c60b3331881"
   },
   "source": [
    "* What can we see from this? Briefly explain the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa663bb-4208-477f-bc50-55f653709ce6",
   "metadata": {
    "id": "8fa663bb-4208-477f-bc50-55f653709ce6"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "The proportion of correct predictions for most categories (such as Class 0, Class 1, Class 2, Class 3, Class 4) is very high (nearly 100%), indicating that the model performs relatively well on these categories.\n",
    "The error rate for Class 5 (in red) is significantly higher than the other classes, indicating that the model performs worse on this class.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c7f51-88d7-42be-8897-b34b7332fe75",
   "metadata": {
    "id": "415c7f51-88d7-42be-8897-b34b7332fe75"
   },
   "source": [
    "* To investigate possible reasons for this imbalance, let's look at our dataset\n",
    "* Plot the train set class distribution as normalized percentages\n",
    "    * You can make use of the Hugging Face datasets object's `to_pandas()` method\n",
    "    * Then you can use all available data plotting and data handling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3617d0-f3e4-4634-8325-2058c636f454",
   "metadata": {
    "id": "ef3617d0-f3e4-4634-8325-2058c636f454"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = tokenized_datasets[\"train\"].to_pandas()\n",
    "\n",
    "label_counts = train_df[\"labels\"].value_counts()\n",
    "label_percentages = (label_counts / label_counts.sum()) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_percentages.plot(kind=\"bar\", color=\"skyblue\", alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Normalized Class Distribution in Training Set\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e98ee-ee8d-4cdd-9b05-8055c257a771",
   "metadata": {
    "id": "940e98ee-ee8d-4cdd-9b05-8055c257a771"
   },
   "source": [
    "1) What problems can you imagine when seeing this class distribution?\n",
    "2) What problems could arise if we simply duplicated samples of the classes with fewer available samples?\n",
    "3) What problems could arise if we simply downsampled the majority classes to the number of minority samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70943ea-f474-469b-9d85-551b88b5da03",
   "metadata": {
    "id": "e70943ea-f474-469b-9d85-551b88b5da03"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "1) the class distribution is extremely uneven. The proportion of Class 5 samples is significantly lower than that of other categories, while Class 0 and Class 1 samples account for the highest proportions. This imbalance can lead to the following problems\n",
    "\n",
    "2) Overfitting to minority classes: Since replicated samples are repetitive in nature, the model may overfit to these samples, resulting in decreased generalization ability for minority classes.\n",
    "Reduced training efficiency: Oversampling increases the size of the dataset, thereby increasing training time and memory requirements.\n",
    "Not addressing data bias: Replicating samples cannot change the bias of the original data, and the feature space of minority classes is still limited, which may make it difficult for the model to learn meaningful features.\n",
    "\n",
    "3) Information loss: Reducing the majority class samples will lead to the loss of important information and reduce the model's ability to learn the characteristics of these categories.\n",
    "Insufficient sample problem: For the downsampled dataset, the overall number of samples is reduced, which may lead to underfitting of the model, especially in complex tasks.\n",
    "The size of the aligned minority class is still insufficient: Even if the majority class is downsampled, the number of samples of the minority class may still be insufficient to capture its complexity characteristics.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e05c3a-0540-4c22-aff7-3642f1c71f3d",
   "metadata": {
    "id": "a1e05c3a-0540-4c22-aff7-3642f1c71f3d"
   },
   "source": [
    "How to deal with this class imbalance?\n",
    "\n",
    "* We can modify the loss function to assign weight coefficients to the losses of certain classes\n",
    "* Therefore, a wrong classification of a certain class is weighted higher or lower, depending on its coefficients we provided\n",
    "* How do we calculate these weights?\n",
    "    * We can make use of the existing normalized frequency distribution we plotted above\n",
    "    * But calculate the element-wise complement per label, i.e. `1 - probability`, so that each resulting label represents a weight between 0 and 1 for its own class\n",
    "    * You can check your calculation by adding element-wise the initial normalized label distribution to your weights, you should receive an array of 6 `1`s (ignoring floating point imprecisions)\n",
    "    * Transform the type to a `torch.float32` tensor, and move the tensor separately to the `device` you want\n",
    "        * Huggingface will use a GPU as soon as it finds one available, but this separate tensor needs to be moved manually\n",
    "* However, since the Hugging Face Trainer we used before abstracted away the option to define our loss function, we need to overwrite the `compute_loss` (see [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.compute_loss)) function of the Trainer\n",
    "* To accomplish that:\n",
    "    *  we inherint from the `Trainer` class\n",
    "    *  define a `compute_loss` function that takes in a `model`, `inputs`, and a `return_outputs=False` keyword\n",
    "    *  First, we calculate the model `outputs` by giving our model the `inputs`\n",
    "        *  our `inputs` consist of a collated batch of inputs that the tokenizer creates, along with a `'labels'`, all inside a dictionary\n",
    "        *  therefore, we need to use dictionary expansion to assign all keyword arguments in the `model`\n",
    "    *  the `outputs` from Hugging Face transformers, again, are a dictionary, so we need to extract their `'logits'` and assign them to a `logits` variable\n",
    "    *  then, we extract the `labels` from the input\n",
    "    *  create a `criterion` using our well known `nn.CrossEntropyLoss()`\n",
    "        *  inside `nn.CrossEntropyLoss()`, we now use the `weight` argument and assign our calculated weight coefficients to it\n",
    "    *  compute the loss\n",
    "    *  return `(loss, outputs) if return_outputs else loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b84fda9-a689-458a-a35a-dbca8dc1affd",
   "metadata": {
    "id": "3b84fda9-a689-458a-a35a-dbca8dc1affd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([0.6649, 0.7084, 0.8651, 0.8789, 0.9185, 0.9643])\n"
     ]
    }
   ],
   "source": [
    "total_samples = label_counts.sum()\n",
    "class_frequencies = label_counts / total_samples\n",
    "\n",
    "complement_weights = 1.0 - class_frequencies\n",
    "\n",
    "complement_weights = torch.tensor(complement_weights.values, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Class Weights:\", complement_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d509e6b5-e09e-4ca3-bf17-3115f2bbc3f7",
   "metadata": {
    "id": "d509e6b5-e09e-4ca3-bf17-3115f2bbc3f7"
   },
   "outputs": [],
   "source": [
    "class BalancedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=complement_weights)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf666a-ffe8-4c09-a2cb-e505c0021788",
   "metadata": {
    "id": "4acf666a-ffe8-4c09-a2cb-e505c0021788"
   },
   "source": [
    "* Now, we can go back and re-create a second `TrainingArguments` instance\n",
    "    * Keep all settings equal, just change the `output_dir` to something like `'./logs/run2'` so that our earlier results aren't overwritten or appended to, and we keep everything in one directory\n",
    "* Re-load a new BERT model, so you start pre-training again from the base pre-trained model\n",
    "* Incorporate the new instance of `TrainingArguments` into our `BalancedLossTrainer`\n",
    "* Run all training, evaluation, and testing steps again\n",
    "* Repeat the plotting of correct and incorrect percentages per class again\n",
    "    * Comment on the new F1 score. Did it change compared to before? Why or why not?\n",
    "    * Comment also on the new results and try to explain what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4441570-c458-4868-a274-eac67e4ef650",
   "metadata": {
    "id": "d4441570-c458-4868-a274-eac67e4ef650"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5088fa3-fbad-4b16-abe1-e0ab9e9d1890",
   "metadata": {
    "id": "a5088fa3-fbad-4b16-abe1-e0ab9e9d1890"
   },
   "outputs": [],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"./logs/run2\",  \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/run2/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,  \n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55f6bbcb-2fb6-4c12-aff9-4cbac1ad0a9e",
   "metadata": {
    "id": "55f6bbcb-2fb6-4c12-aff9-4cbac1ad0a9e"
   },
   "outputs": [],
   "source": [
    "trainer = BalancedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args2,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c722db60-ead1-4222-b1e7-0d3c662100ff",
   "metadata": {
    "id": "c722db60-ead1-4222-b1e7-0d3c662100ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 109486854\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 12:10:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.237667</td>\n",
       "      <td>0.931035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.136471</td>\n",
       "      <td>0.945414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.088500</td>\n",
       "      <td>0.181142</td>\n",
       "      <td>0.944163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./logs/run2\\checkpoint-1000\n",
      "Configuration saved in ./logs/run2\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./logs/run2\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run2\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run2\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./logs/run2\\checkpoint-2000\n",
      "Configuration saved in ./logs/run2\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./logs/run2\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run2\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run2\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./logs/run2\\checkpoint-3000\n",
      "Configuration saved in ./logs/run2\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./logs/run2\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run2\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run2\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [logs\\run2\\checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./logs/run2\\checkpoint-2000 (score: 0.945414062119489).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:2024: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.2186982831954956, metrics={'train_runtime': 43901.7479, 'train_samples_per_second': 1.093, 'train_steps_per_second': 0.068, 'total_flos': 1.2629784231936e+16, 'train_loss': 0.2186982831954956, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d79524c1-0b42-4bfd-ace4-0cb786d2c428",
   "metadata": {
    "id": "d79524c1-0b42-4bfd-ace4-0cb786d2c428"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'test_loss': 0.18027490377426147, 'test_f1': 0.9313537648053397, 'test_runtime': 503.8331, 'test_samples_per_second': 3.97, 'test_steps_per_second': 0.496}\n"
     ]
    }
   ],
   "source": [
    "test_predictions = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9b0d240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 0.9313537648053397\n"
     ]
    }
   ],
   "source": [
    "print(\"Test F1 Score:\", test_predictions.metrics[\"test_f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83faaec0-6cb9-4956-940e-d00d25469756",
   "metadata": {
    "id": "83faaec0-6cb9-4956-940e-d00d25469756"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOP0lEQVR4nO3deVhUdf8+8HtkGXZQFBACRATF3dwSUyAFVzDN3BV3DS1xw3ADLOGRTCnNtQQzzbLUr9mjSa65o4ipmCvigoQLggvr8Pn94Y95HAGFYXSG4/3qmutqPmeZ93nPALdnmSMTQggQERERSVQ1bRdARERE9Cox7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsUKX9/fffGDFiBFxcXGBkZAQzMzO8/fbbiI6Oxv3797VdXoWlpaUhPDwcSUlJ2i4FABAXFweZTIYTJ05ouxStS05ORnh4OK5du1au+Yt7V/zQ19fHW2+9hREjRuDWrVuvttj/r06dOhg+fLjy+b59+yCTybBv374Krefw4cMIDw/HgwcPSkzz9vaGt7d3pep8E5T3dxX7KT362i6AqrbVq1cjKCgI9evXx/Tp09GwYUMUFBTgxIkTWLFiBY4cOYItW7Zou8wKSUtLQ0REBOrUqYPmzZtruxx6RnJyMiIiIuDt7Y06deqUe7nY2Fg0aNAAOTk5OHDgAKKiorB//36cOXMGpqamr67gUrz99ts4cuQIGjZsWKHlDh8+jIiICAwfPhxWVlYq05YtW6bBCqVJir+rqPwYdkhtR44cwUcffQRfX19s3boVcrlcOc3X1xdTp07Fzp07NfJaT548gYmJSYlxhUKBwsJCldem1ysnJwfGxsYlxgsKCpR7U7StcePGaNWqFQDAx8cHCoUCn332GbZu3YrBgweXukxZn7nKsrCwwDvvvKPRdVY0OEnRi96v1/m7inQTD2OR2iIjIyGTybBq1apSw4ahoSECAgKUz4uKihAdHY0GDRpALpfDxsYGw4YNw82bN1WW8/b2RuPGjXHgwAF4enrCxMQEI0eOxLVr1yCTyRAdHY3PP/8cLi4ukMvl2Lt3LwDgxIkTCAgIQI0aNWBkZIQWLVrg559/LlHXrVu3MHbsWDg6OsLQ0BD29vbo27cv/v33X+zbtw+tW7cGAIwYMUJ5+CM8PLzUHpw+fRoymQzfffddiWk7duyATCbDtm3bAAB37txRvq5cLketWrXQvn17/Pnnn+Vr+DOGDx8OMzMzXL58Gd27d4eZmRkcHR0xdepU5OXlqcybl5eHefPmwcPDA0ZGRrC2toaPjw8OHz6snCc3NxehoaFwcXGBoaEhHBwcMGHChBKHTOrUqYOePXti8+bNaNGiBYyMjBAREaE8NLNu3TpMnToVDg4OkMvluHz5MgDgzz//RKdOnWBhYQETExO0b98eu3fvLrFd//zzDwYOHAhbW1vI5XI4OTlh2LBhyMvLQ1xcHD788EMATwNL8XsTFxdX4f4Vh43U1FSVfp45cwZ+fn4wNzdHp06dAAD5+fn4/PPPlZ/bWrVqYcSIEbhz547KOgsKChASEgI7OzuYmJjg3XffxfHjx0u8dlmHsY4dOwZ/f39YW1vDyMgIrq6uCA4OBgCEh4dj+vTpAAAXFxflthevo7TDLvfv30dQUBAcHBxgaGiIunXrYtasWSU+HzKZDBMnTsS6devg4eEBExMTNGvWDNu3b1eZT93Pb3h4OGQyGU6dOoU+ffrAwsIClpaWGDJkSIkeAsBPP/2Edu3awdTUFGZmZujSpQtOnTqlMs+L3q/SVPR3VWkiIiLQtm1b1KhRAxYWFnj77bfx3Xff4fl7ae/Zswfe3t6wtraGsbExnJyc8MEHH+DJkyfKeZYvX45mzZrBzMwM5ubmaNCgAWbOnPnC16fK0f4/uahKUigU2LNnD1q2bAlHR8dyLfPRRx9h1apVmDhxInr27Ilr165hzpw52LdvHxITE1GzZk3lvLdv38aQIUMQEhKCyMhIVKv2v1z+9ddfw93dHQsXLoSFhQXc3Nywd+9edO3aFW3btsWKFStgaWmJjRs3on///njy5InynIlbt26hdevWKCgowMyZM9G0aVPcu3cPf/zxBzIzM/H2228jNjYWI0aMwOzZs9GjRw8AwFtvvVXqNjVr1gwtWrRAbGwsRo0apTItLi4ONjY26N69OwBg6NChSExMxPz58+Hu7o4HDx4gMTER9+7dK3ffn1VQUICAgACMGjUKU6dOxYEDB/DZZ5/B0tISc+fOBQAUFhaiW7du+OuvvxAcHIz33nsPhYWFOHr0KK5fvw5PT08IIfD+++9j9+7dCA0NRYcOHfD3338jLCwMR44cwZEjR1T+QCQmJuL8+fOYPXs2XFxcYGpqisePHwMAQkND0a5dO6xYsQLVqlWDjY0NfvjhBwwbNgy9evXC2rVrYWBggJUrV6JLly74448/lH+kTp8+jXfffRc1a9bEvHnz4Obmhtu3b2Pbtm3Iz89Hjx49EBkZiZkzZ+Kbb77B22+/DQBwdXWtcO+KQ1itWrWUY/n5+QgICMC4cePw6aeforCwEEVFRejVqxf++usvhISEwNPTE6mpqQgLC4O3tzdOnDih3Ks1ZswYfP/995g2bRp8fX1x9uxZ9OnTBw8fPnxpPX/88Qf8/f3h4eGBRYsWwcnJCdeuXcOuXbsAAKNHj8b9+/exZMkSbN68GbVr1wZQ9h6d3Nxc+Pj44MqVK4iIiEDTpk3x119/ISoqCklJSfj9999V5v/999+RkJCAefPmwczMDNHR0ejduzcuXLiAunXrAqj857d3797o168fxo8fj3PnzmHOnDlITk7GsWPHYGBgAOBpKJk9e7by5y8/Px9ffPEFOnTogOPHj6tsb2nvV2nU+V1VmmvXrmHcuHFwcnICABw9ehQff/wxbt26pfx5u3btGnr06IEOHTpgzZo1sLKywq1bt7Bz507k5+fDxMQEGzduRFBQED7++GMsXLgQ1apVw+XLl5GcnKx2bVQOgkgN6enpAoAYMGBAueY/f/68ACCCgoJUxo8dOyYAiJkzZyrHvLy8BACxe/dulXlTUlIEAOHq6iry8/NVpjVo0EC0aNFCFBQUqIz37NlT1K5dWygUCiGEECNHjhQGBgYiOTm5zFoTEhIEABEbG1uubfv6668FAHHhwgXl2P3794VcLhdTp05VjpmZmYng4OByrfNZsbGxAoBISEhQjgUGBgoA4ueff1aZt3v37qJ+/frK599//70AIFavXl3m+nfu3CkAiOjoaJXxn376SQAQq1atUo45OzsLPT09lW0VQoi9e/cKAKJjx44q448fPxY1atQQ/v7+KuMKhUI0a9ZMtGnTRjn23nvvCSsrK5GRkVFmrZs2bRIAxN69e8uc51nFvTt69KgoKCgQDx8+FNu3bxe1atUS5ubmIj09XQjxv36uWbNGZfkff/xRABC//vqrynjxZ2TZsmVCiP99vidPnqwy3/r16wUAERgYqBwr7tWz2+Dq6ipcXV1FTk5OmdvyxRdfCAAiJSWlxDQvLy/h5eWlfL5ixYpSPx8LFiwQAMSuXbuUYwCEra2tyM7OVo6lp6eLatWqiaioKOWYup/fsLCwF/bmhx9+EEIIcf36daGvry8+/vhjlfkePnwo7OzsRL9+/ZRjZb1fpano7yohSvbzeQqFQhQUFIh58+YJa2trUVRUJIQQ4pdffhEARFJSUpnLTpw4UVhZWZW7FtIMHsai16L4UNOzV6UAQJs2beDh4VHikEb16tXx3nvvlbqugIAA5b8Egaf/Sv/nn3+U514UFhYqH927d8ft27dx4cIFAE8PLfn4+MDDw0NTm4bBgwdDLperHE758ccfkZeXhxEjRijH2rRpg7i4OHz++ec4evQoCgoKKvW6MpkM/v7+KmNNmzZVHpoBnm6vkZERRo4cWeZ69uzZA6Dke/Phhx/C1NS0xHvTtGlTuLu7l7quDz74QOX54cOHcf/+fQQGBqq8L0VFRejatSsSEhLw+PFjPHnyBPv370e/fv1U9rZoyjvvvAMDAwOYm5ujZ8+esLOzw44dO2Bra/vC+rdv3w4rKyv4+/ur1N+8eXPY2dkpDyMVf76fP/+nX79+Lz1n6eLFi7hy5QpGjRoFIyOjSm7pU3v27IGpqSn69u2rMl78Hj//nvr4+MDc3Fz53NbWFjY2Niqfpcp+fsvqTXHv/vjjDxQWFmLYsGEqvTYyMoKXl1epV689/369Snv27EHnzp1haWkJPT09GBgYYO7cubh37x4yMjIAAM2bN4ehoSHGjh2LtWvX4urVqyXW06ZNGzx48AADBw7E//3f/+Hu3buvbRveZAw7pJaaNWvCxMQEKSkp5Zq/eFd38e73Z9nb25fYFV7afGVN+/fffwEA06ZNg4GBgcojKCgIAJS/UO7cuVPmISl11ahRAwEBAfj++++hUCgAPD2E1aZNGzRq1Eg5308//YTAwEB8++23aNeuHWrUqIFhw4YhPT1drdc1MTEp8cdRLpcjNzdX+fzOnTuwt7dXOQz4vHv37kFfX79EyJDJZLCzs9PIe9O3b98S782CBQsghMD9+/eRmZkJhUKh8fem2Pfff4+EhAScOnUKaWlp+Pvvv9G+fXuVeUxMTGBhYVGi/gcPHsDQ0LBE/enp6crPVXGP7OzsVJbX19eHtbX1C2srPm9Fk9t+79492NnZQSaTqYzb2NhAX1+/xHtaWo1yuRw5OTnK55X9/JbVm+Jaij8rrVu3LtHrn376qUQoKO39Kk1Ff1eV5vjx4/Dz8wPw9KquQ4cOISEhAbNmzQIAZZ9cXV3x559/wsbGBhMmTICrqytcXV3x1VdfKdc1dOhQrFmzBqmpqfjggw9gY2ODtm3bIj4+Xu366OV4zg6pRU9PD506dcKOHTtw8+bNl/6iLv5levv27RLzpqWlqZyvA6DEL+kXTSteNjQ0FH369Cl1mfr16wN4eo7G8ydEa8KIESOwadMmxMfHw8nJCQkJCVi+fHmJOmNiYhATE4Pr169j27Zt+PTTT5GRkfHKrgSpVasWDh48iKKiojIDj7W1NQoLC3Hnzh2VwCOEQHp6uvKE7WLqvDdLliwp8wokW1tbKBQK6OnpvZL3BgA8PDyUV2OVpbTtqlmzJqytrct8f4r3hhR/vtPT0+Hg4KCcXlhY+NJzWop7rsltt7a2xrFjxyCEUNmujIwMFBYWlvh5K4/Kfn7L6k1x74pr+uWXX+Ds7PzS9b3oc/isiv6uKs3GjRthYGCA7du3q/wDY+vWrSXm7dChAzp06ACFQoETJ05gyZIlCA4Ohq2tLQYMGADg6e+LESNG4PHjxzhw4ADCwsLQs2dPXLx4sVzbThXHPTukttDQUAghMGbMGOTn55eYXlBQgN9++w0AlIekfvjhB5V5EhIScP78+RdeSfEy9evXh5ubG06fPo1WrVqV+ij+o9StWzfs3btXeVirNMUn4z77r9qX8fPzg4ODA2JjYxEbGwsjIyMMHDiwzPmdnJwwceJE+Pr6IjExsdyvU1HdunVDbm7uC69YKu798+/Nr7/+isePH1fqvWnfvj2srKyQnJxc5ntjaGgIY2NjeHl5YdOmTS/cra/Oe1MZPXv2xL1796BQKEqtvThEF18JtX79epXlf/755zJPnC3m7u4OV1dXrFmzpsSVUs+qyLZ36tQJjx49KvHH+Pvvv1dOrwx1Pr9l9aa4d126dIG+vj6uXLlS5mdFXRX5XVWa4q9Q0NPTU47l5ORg3bp1ZS6jp6eHtm3b4ptvvgGAUvtkamqKbt26YdasWcjPz8e5c+cqsllUAdyzQ2pr164dli9fjqCgILRs2RIfffQRGjVqhIKCApw6dQqrVq1C48aN4e/vj/r162Ps2LFYsmQJqlWrhm7duimvxnJ0dMTkyZMrVcvKlSvRrVs3dOnSBcOHD4eDgwPu37+P8+fPIzExEZs2bQIAzJs3Dzt27EDHjh0xc+ZMNGnSBA8ePMDOnTsxZcoUNGjQAK6urjA2Nsb69evh4eEBMzMz2Nvbw97evszX19PTw7Bhw7Bo0SJYWFigT58+sLS0VE7PysqCj48PBg0ahAYNGsDc3BwJCQnYuXNnmXujNGHgwIGIjY3F+PHjceHCBfj4+KCoqAjHjh2Dh4cHBgwYAF9fX3Tp0gUzZsxAdnY22rdvr7waq0WLFhg6dKjar29mZoYlS5YgMDAQ9+/fR9++fWFjY4M7d+7g9OnTuHPnjnIP2KJFi/Duu++ibdu2+PTTT1GvXj38+++/2LZtG1auXAlzc3M0btwYALBq1SqYm5vDyMgILi4uLz1UpK4BAwZg/fr16N69OyZNmoQ2bdrAwMAAN2/exN69e9GrVy/07t0bHh4eGDJkCGJiYmBgYIDOnTvj7NmzyisGX+abb76Bv78/3nnnHUyePBlOTk64fv06/vjjD2VIaNKkCQDgq6++QmBgIAwMDFC/fn2Vc22KDRs2DN988w0CAwNx7do1NGnSBAcPHkRkZCS6d++Ozp07V6gPmvj8bt68Gfr6+vD19VVejdWsWTP069cPwNOvNZg3bx5mzZqFq1evomvXrqhevTr+/fdfHD9+HKampoiIiKhQ3cUq8ruqND169MCiRYswaNAgjB07Fvfu3cPChQtLXMa+YsUK7NmzBz169ICTkxNyc3OxZs0aAFD2fMyYMTA2Nkb79u1Ru3ZtpKenIyoqCpaWliX2opIGafX0aJKEpKQkERgYKJycnIShoaEwNTUVLVq0EHPnzlW5skahUIgFCxYId3d3YWBgIGrWrCmGDBkibty4obI+Ly8v0ahRoxKvU3w11hdffFFqHadPnxb9+vUTNjY2wsDAQNjZ2Yn33ntPrFixQmW+GzduiJEjRwo7OzthYGAg7O3tRb9+/cS///6rnOfHH38UDRo0EAYGBgKACAsLe2kfLl68KAAIACI+Pl5lWm5urhg/frxo2rSpsLCwEMbGxqJ+/foiLCxMPH78+IXrLetqLFNT0xLzFl/58qycnBwxd+5c4ebmJgwNDYW1tbV47733xOHDh1XmmTFjhnB2dhYGBgaidu3a4qOPPhKZmZkq63J2dhY9evQo8brFVxht2rSp1G3Yv3+/6NGjh6hRo4YwMDAQDg4OokePHiXmT05OFh9++KGwtrYWhoaGwsnJSQwfPlzk5uYq54mJiREuLi5CT0/vpVfNlda70pTVTyGEKCgoEAsXLhTNmjUTRkZGwszMTDRo0ECMGzdOXLp0STlfXl6emDp1qrCxsRFGRkbinXfeEUeOHBHOzs4vvRpLCCGOHDkiunXrJiwtLYVcLheurq4lrmAKDQ0V9vb2olq1airrKO3qoXv37onx48eL2rVrC319feHs7CxCQ0NVeinE06uxJkyYUGK7n627Mp/f4s/kyZMnhb+/vzAzMxPm5uZi4MCBKj9zxbZu3Sp8fHyEhYWFkMvlwtnZWfTt21f8+eefynle9H69SHl/V5XWzzVr1oj69esLuVwu6tatK6KiosR3332ncoXckSNHRO/evYWzs7OQy+XC2tpaeHl5iW3btinXs3btWuHj4yNsbW2FoaGh8vfP33//XeHtofKTCfHcNyIRERFpSHh4OCIiInDnzh21zhUi0gSes0NERESSxrBDREREksbDWERERCRp3LNDREREksawQ0RERJLGsENERESSxi8VBFBUVIS0tDSYm5uX+yvIiYiISLuEEHj48OFL7wHIsIOn92ZydHTUdhlERESkhhs3brzwvmcMO/jfzfxu3LhRrq92JyIiIu3Lzs6Go6NjqbdNeRbDDv5391wLCwuGHSIioirmZaeg8ARlIiIikjSGHSIiIpI0hh0iIiKSNIYdIiIikjSGHSIiIpI0hh0iIiKSNIYdIiIikjSGHSIiIpI0hh0iIiKSNIYdIiIikjSthp0DBw7A398f9vb2kMlk2Lp1q8p0IQTCw8Nhb28PY2NjeHt749y5cyrz5OXl4eOPP0bNmjVhamqKgIAA3Lx58zVuBREREekyrYadx48fo1mzZli6dGmp06Ojo7Fo0SIsXboUCQkJsLOzg6+vLx4+fKicJzg4GFu2bMHGjRtx8OBBPHr0CD179oRCoXhdm0FEREQ6TCaEENouAnh6E68tW7bg/fffB/B0r469vT2Cg4MxY8YMAE/34tja2mLBggUYN24csrKyUKtWLaxbtw79+/cHAKSlpcHR0RH//e9/0aVLl3K9dnZ2NiwtLZGVlcUbgRIREVUR5f37rbPn7KSkpCA9PR1+fn7KMblcDi8vLxw+fBgAcPLkSRQUFKjMY29vj8aNGyvnISIiojebvrYLKEt6ejoAwNbWVmXc1tYWqampynkMDQ1RvXr1EvMUL1+avLw85OXlKZ9nZ2drqmwiIiLSMTobdorJZDKV50KIEmPPe9k8UVFRiIiI0Eh9L/WSWiWtMkdI39S+VfaoMvumHvat4t7UngHsmzq0fMaMzh7GsrOzA4ASe2gyMjKUe3vs7OyQn5+PzMzMMucpTWhoKLKyspSPGzduaLh6IiIi0hU6G3ZcXFxgZ2eH+Ph45Vh+fj72798PT09PAEDLli1hYGCgMs/t27dx9uxZ5TylkcvlsLCwUHkQERGRNGn1MNajR49w+fJl5fOUlBQkJSWhRo0acHJyQnBwMCIjI+Hm5gY3NzdERkbCxMQEgwYNAgBYWlpi1KhRmDp1KqytrVGjRg1MmzYNTZo0QefOnbW1WURERKRDtBp2Tpw4AR8fH+XzKVOmAAACAwMRFxeHkJAQ5OTkICgoCJmZmWjbti127doFc3Nz5TKLFy+Gvr4++vXrh5ycHHTq1AlxcXHQ09N77dtDREREukdnvmdHm17p9+y8qSejATyJTx080VY97Jt6+DOqHvat4l5R1Kjy37NDREREpAkMO0RERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaVq9XQQRaZYsXNsVaEdlv5uVfSOSNu7ZISIiIknjnh0iIqqwN3VvGMA9YlURw84rxl8IRERE2sXDWERERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaQw7REREJGn8BmXSSW/qN0/zW6eJiDSPe3aIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0nQ67BQWFmL27NlwcXGBsbEx6tati3nz5qGoqEg5jxAC4eHhsLe3h7GxMby9vXHu3DktVk1ERES6RKfDzoIFC7BixQosXboU58+fR3R0NL744gssWbJEOU90dDQWLVqEpUuXIiEhAXZ2dvD19cXDhw+1WDkRERHpCp0OO0eOHEGvXr3Qo0cP1KlTB3379oWfnx9OnDgB4OlenZiYGMyaNQt9+vRB48aNsXbtWjx58gQbNmzQcvVERESkC3Q67Lz77rvYvXs3Ll68CAA4ffo0Dh48iO7duwMAUlJSkJ6eDj8/P+UycrkcXl5eOHz4cJnrzcvLQ3Z2tsqDiIiIpElf2wW8yIwZM5CVlYUGDRpAT08PCoUC8+fPx8CBAwEA6enpAABbW1uV5WxtbZGamlrmeqOiohAREfHqCiciIiKdodN7dn766Sf88MMP2LBhAxITE7F27VosXLgQa9euVZlPJpOpPBdClBh7VmhoKLKyspSPGzduvJL6iYiISPt0es/O9OnT8emnn2LAgAEAgCZNmiA1NRVRUVEIDAyEnZ0dgKd7eGrXrq1cLiMjo8TenmfJ5XLI5fJXWzwRERHpBJ3es/PkyRNUq6Zaop6envLScxcXF9jZ2SE+Pl45PT8/H/v374enp+drrZWIiIh0k07v2fH398f8+fPh5OSERo0a4dSpU1i0aBFGjhwJ4Onhq+DgYERGRsLNzQ1ubm6IjIyEiYkJBg0apOXqiYiISBfodNhZsmQJ5syZg6CgIGRkZMDe3h7jxo3D3LlzlfOEhIQgJycHQUFByMzMRNu2bbFr1y6Ym5trsXIiIiLSFToddszNzRETE4OYmJgy55HJZAgPD0d4ePhrq4uIiIiqDp0+Z4eIiIioshh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjSGHaIiIhI0hh2iIiISNIYdoiIiEjS9LVdABER0ZtCFq7tCrRDaPn1uWeHiIiIJI1hh4iIiCSNYYeIiIgkjWGHiIiIJI1hh4iIiCSNYYeIiIgkjWGHiIiIJI1hh4iIiCRN7bBz5coVzJ49GwMHDkRGRgYAYOfOnTh37pzGiiMiIiKqLLXCzv79+9GkSRMcO3YMmzdvxqNHjwAAf//9N8LCwjRaIBEREVFlqBV2Pv30U3z++eeIj4+HoaGhctzHxwdHjhzRWHFERERElaVW2Dlz5gx69+5dYrxWrVq4d+9epYsiIiIi0hS1wo6VlRVu375dYvzUqVNwcHCodFFEREREmqJW2Bk0aBBmzJiB9PR0yGQyFBUV4dChQ5g2bRqGDRum0QJv3bqFIUOGwNraGiYmJmjevDlOnjypnC6EQHh4OOzt7WFsbAxvb2+eJE1ERERKaoWd+fPnw8nJCQ4ODnj06BEaNmyIjh07wtPTE7Nnz9ZYcZmZmWjfvj0MDAywY8cOJCcn48svv4SVlZVynujoaCxatAhLly5FQkIC7Ozs4Ovri4cPH2qsDiIiIqq69NVZyMDAAOvXr8e8efNw6tQpFBUVoUWLFnBzc9NocQsWLICjoyNiY2OVY3Xq1FH+vxACMTExmDVrFvr06QMAWLt2LWxtbbFhwwaMGzdOo/UQERFR1VOpLxV0dXVF37590a9fP40HHQDYtm0bWrVqhQ8//BA2NjZo0aIFVq9erZyekpKC9PR0+Pn5Kcfkcjm8vLxw+PBhjddDREREVY9ae3amTJlS6rhMJoORkRHq1auHXr16oUaNGpUq7urVq1i+fDmmTJmCmTNn4vjx4/jkk08gl8sxbNgwpKenAwBsbW1VlrO1tUVqamqZ683Ly0NeXp7yeXZ2dqXqJCIiIt2lVtg5deoUEhMToVAoUL9+fQghcOnSJejp6aFBgwZYtmwZpk6dioMHD6Jhw4ZqF1dUVIRWrVohMjISANCiRQucO3cOy5cvVzkRWiaTqSwnhCgx9qyoqChERESoXRcRERFVHWodxurVqxc6d+6MtLQ0nDx5EomJibh16xZ8fX0xcOBA3Lp1Cx07dsTkyZMrVVzt2rVLhCUPDw9cv34dAGBnZwcAyj08xTIyMkrs7XlWaGgosrKylI8bN25Uqk4iIiLSXWqFnS+++AKfffYZLCwslGMWFhYIDw9HdHQ0TExMMHfuXJVLxNXRvn17XLhwQWXs4sWLcHZ2BgC4uLjAzs4O8fHxyun5+fnYv38/PD09y1yvXC6HhYWFyoOIiIikSa2wk5WVpbz557Pu3LmjPP/FysoK+fn5lSpu8uTJOHr0KCIjI3H58mVs2LABq1atwoQJEwA8PXwVHByMyMhIbNmyBWfPnsXw4cNhYmKCQYMGVeq1iYiISBrUOmenV69eGDlyJL788ku0bt0aMpkMx48fx7Rp0/D+++8DAI4fPw53d/dKFde6dWts2bIFoaGhmDdvHlxcXBATE4PBgwcr5wkJCUFOTg6CgoKQmZmJtm3bYteuXTA3N6/UaxMREZE0qBV2Vq5cicmTJ2PAgAEoLCx8uiJ9fQQGBmLx4sUAgAYNGuDbb7+tdIE9e/ZEz549y5wuk8kQHh6O8PDwSr8WERERSY9aYcfMzAyrV6/G4sWLcfXqVQgh4OrqCjMzM+U8zZs311SNRERERGpTK+wUMzMzQ9OmTTVVCxEREZHGqR12EhISsGnTJly/fr3EicibN2+udGFEREREmqDW1VgbN25E+/btkZycjC1btqCgoADJycnYs2cPLC0tNV0jERERkdrUCjuRkZFYvHgxtm/fDkNDQ3z11Vc4f/48+vXrBycnJ03XSERERKQ2tcLOlStX0KNHDwBPv6Dv8ePHkMlkmDx5MlatWqXRAomIiIgqQ62wU6NGDTx8+BAA4ODggLNnzwIAHjx4gCdPnmiuOiIiIqJKUusE5Q4dOiA+Ph5NmjRBv379MGnSJOzZswfx8fHo1KmTpmskIiIiUptaYWfp0qXIzc0F8PSmmgYGBjh48CD69OmDOXPmaLRAIiIiospQK+zUqFFD+f/VqlVDSEgIQkJCNFYUERERkaaodc6Onp5eqTcCvXfvHvT09CpdFBEREZGmqBV2hBCljufl5cHQ0LBSBRERERFpUoUOY3399dcAnt5889tvv1W5F5ZCocCBAwfQoEEDzVZIREREVAkVCjvFdzQXQmDFihUqh6wMDQ1Rp04drFixQrMVEhEREVVChcJOSkoKAMDHxwebN29G9erVX0lRRERERJqi1tVYe/fu1XQdRERERK+EWmFHoVAgLi4Ou3fvRkZGBoqKilSm79mzRyPFEREREVWWWmFn0qRJiIuLQ48ePdC4cWPIZDJN10VERESkEWqFnY0bN+Lnn39G9+7dNV0PERERkUap9T07hoaGqFevnqZrISIiItI4tcLO1KlT8dVXX5X55YJEREREukKtw1gHDx7E3r17sWPHDjRq1AgGBgYq0zdv3qyR4oiIiIgqS62wY2Vlhd69e2u6FiIiIiKNUyvsxMbGaroOIiIioldCrXN2AKCwsBB//vknVq5ciYcPHwIA0tLS8OjRI40VR0RERFRZau3ZSU1NRdeuXXH9+nXk5eXB19cX5ubmiI6ORm5uLu+PRURERDpDrT07kyZNQqtWrZCZmQljY2PleO/evbF7926NFUdERERUWWpfjXXo0CEYGhqqjDs7O+PWrVsaKYyIiIhIE9Tas1NUVASFQlFi/ObNmzA3N690UURERESaolbY8fX1RUxMjPK5TCbDo0ePEBYWxltIEBERkU5R6zDW4sWL4ePjg4YNGyI3NxeDBg3CpUuXULNmTfz444+arpGIiIhIbWqFHXt7eyQlJWHjxo04efIkioqKMGrUKAwePFjlhGUiIiIibVMr7ACAsbExRowYgREjRmiyHiIiIiKNUuucnaioKKxZs6bE+Jo1a7BgwYJKF0VERESkKWqFnZUrV6JBgwYlxhs1asQvFCQiIiKdolbYSU9PR+3atUuM16pVC7dv3650UURERESaolbYcXR0xKFDh0qMHzp0CPb29pUuioiIiEhT1DpBefTo0QgODkZBQQHee+89AMDu3bsREhKCqVOnarRAIiIiospQK+yEhITg/v37CAoKQn5+PgDAyMgIM2bMQGhoqEYLJCIiIqqMCocdhUKBgwcPYsaMGZgzZw7Onz8PY2NjuLm5QS6Xv4oaiYiIiNRW4bCjp6eHLl264Pz583BxcUHr1q1fRV1EREREGqHWCcpNmjTB1atXNV0LERERkcapFXbmz5+PadOmYfv27bh9+zays7NVHkRERES6Qq0TlLt27QoACAgIgEwmU44LISCTyaBQKDRTHREREVElqRV29u7dq+k6iIiIiF4JtcKOl5eXpusgIiIieiXUOmcHAP766y8MGTIEnp6euHXrFgBg3bp1OHjwoMaKIyIiIqostcLOr7/+ii5dusDY2BiJiYnIy8sDADx8+BCRkZEaLZCIiIioMtQKO59//jlWrFiB1atXw8DAQDnu6emJxMREjRVHREREVFlqhZ0LFy6gY8eOJcYtLCzw4MGDytZEREREpDFqhZ3atWvj8uXLJcYPHjyIunXrVrooIiIiIk1RK+yMGzcOkyZNwrFjxyCTyZCWlob169dj2rRpCAoK0nSNRERERGpT+67n2dnZ8PHxQW5uLjp27Ai5XI5p06Zh4sSJmq6RiIiISG0VCjtPnjzB9OnTsXXrVhQUFMDf3x9Tp04FADRs2BBmZmavpEgiIiIidVUo7ISFhSEuLg6DBw+GsbExNmzYgKKiImzatOlV1UdERERUKRUKO5s3b8Z3332HAQMGAAAGDx6M9u3bQ6FQQE9P75UUSERERFQZFTpB+caNG+jQoYPyeZs2baCvr4+0tDSNF0ZERESkCRUKOwqFAoaGhipj+vr6KCws1GhRRERERJpSocNYQggMHz4ccrlcOZabm4vx48fD1NRUObZ582bNVUhERERUCRUKO4GBgSXGhgwZorFiiIiIiDStQmEnNjb2VdVRLlFRUZg5cyYmTZqEmJgYAE/3NkVERGDVqlXIzMxE27Zt8c0336BRo0ZarZWIiIh0g1rfoKwNCQkJWLVqFZo2baoyHh0djUWLFmHp0qVISEiAnZ0dfH198fDhQy1VSkRERLqkSoSdR48eYfDgwVi9ejWqV6+uHBdCICYmBrNmzUKfPn3QuHFjrF27Fk+ePMGGDRu0WDERERHpiioRdiZMmIAePXqgc+fOKuMpKSlIT0+Hn5+fckwul8PLywuHDx8uc315eXnIzs5WeRAREZE0qXVvrNdp48aNOHnyJE6cOFFiWnp6OgDA1tZWZdzW1hapqallrjMqKgoRERGaLZSIiIh0kk7v2blx4wYmTZqE9evXw8jIqMz5ZDKZynMhRImxZ4WGhiIrK0v5uHHjhsZqJiIiIt2i03t2Tp48iYyMDLRs2VI5plAocODAASxduhQXLlwA8HQPT+3atZXzZGRklNjb8yy5XK7yXUFEREQkXTq9Z6dTp044c+YMkpKSlI9WrVph8ODBSEpKQt26dWFnZ4f4+HjlMvn5+di/fz88PT21WDkRERHpCp3es2Nubo7GjRurjJmamsLa2lo5HhwcjMjISLi5ucHNzQ2RkZEwMTHBoEGDtFEyERER6RidDjvlERISgpycHAQFBSm/VHDXrl0wNzfXdmlERESkA6pc2Nm3b5/Kc5lMhvDwcISHh2ulHiIiItJtOn3ODhEREVFlMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpDHsEBERkaQx7BAREZGkMewQERGRpOl02ImKikLr1q1hbm4OGxsbvP/++7hw4YLKPEIIhIeHw97eHsbGxvD29sa5c+e0VDERERHpGp0OO/v378eECRNw9OhRxMfHo7CwEH5+fnj8+LFynujoaCxatAhLly5FQkIC7Ozs4Ovri4cPH2qxciIiItIV+tou4EV27typ8jw2NhY2NjY4efIkOnbsCCEEYmJiMGvWLPTp0wcAsHbtWtja2mLDhg0YN26cNsomIiIiHaLTe3ael5WVBQCoUaMGACAlJQXp6enw8/NTziOXy+Hl5YXDhw+XuZ68vDxkZ2erPIiIiEiaqkzYEUJgypQpePfdd9G4cWMAQHp6OgDA1tZWZV5bW1vltNJERUXB0tJS+XB0dHx1hRMREZFWVZmwM3HiRPz999/48ccfS0yTyWQqz4UQJcaeFRoaiqysLOXjxo0bGq+XiIiIdINOn7NT7OOPP8a2bdtw4MABvPXWW8pxOzs7AE/38NSuXVs5npGRUWJvz7PkcjnkcvmrK5iIiIh0hk7v2RFCYOLEidi8eTP27NkDFxcXlekuLi6ws7NDfHy8ciw/Px/79++Hp6fn6y6XiIiIdJBO79mZMGECNmzYgP/7v/+Dubm58jwcS0tLGBsbQyaTITg4GJGRkXBzc4ObmxsiIyNhYmKCQYMGabl6IiIi0gU6HXaWL18OAPD29lYZj42NxfDhwwEAISEhyMnJQVBQEDIzM9G2bVvs2rUL5ubmr7laIiIi0kU6HXaEEC+dRyaTITw8HOHh4a++ICIiIqpydPqcHSIiIqLKYtghIiIiSWPYISIiIklj2CEiIiJJY9ghIiIiSWPYISIiIklj2CEiIiJJY9ghIiIiSWPYISIiIklj2CEiIiJJ0+nbRegahUKBgoKCCi3jbOr8iqrRfbm5uWovW9m+FaEIt5/cRqEorNR6iIio6mPYKQchBNLT0/HgwYMKL7ui/QrNF1RFpKSkqL1sZfsmIHA35y6mJkzFndw7lVoXERFVbQw75VAcdGxsbGBiYgKZTFbuZR9nPH6Flek2FxsXtZetdN8EYJ5pjo/qf4TPTn8GgZffVJaIiKSJYeclFAqFMuhYW1tXfAVvcIeNjIzUX1gDfTOyNEKrWq1gaWiJB/kPKr9CIiKqkniC8ksUn6NjYmKi5UqowqoB+tX0YWFgoe1KiIhIixh2yqkih65IR8gA2f//j4iI3lwMO0RERCRpDDsSdzfjLr6Y/QV6tesFTxdP9GjVA5MDJ+P4X8e1XVoJcXFxsLKy0nYZREQkMW/w6bOVI4t4vYdGEsYkVHiZtBtpGP3+aJhZmOGTWZ+gnkc9FBYW4ui+o4ieFY1fDvxS4XUWFhRC36Dkx6ascSIiIm3jnh0JWzBzAWSQYe3va9GpZyc4uzrDtb4rBo8bjNjfYgEA6bfSMXXEVHR06wjv+t4IHReKe3fuKdex6stVGOQ7CNs2blPuHRJCoLVDa/z6/a+YOmIqOtTrgO+++g4AcGDXAQztOhTt67ZH3bp1ERERgcLC/32x34MHDzB27FjY2trCyMgIjRs3xvbt27Fv3z6MGDECWVlZkMlkaO3QGqu+XPV6G0ZERJLEf4pLVFZmFo7sPYKPZnwEYxPjEtPNLc0hhMC0kdNgbGKMlb+uhKJQgQUzF2DmRzOx8peVynlvXruJ+N/iEb06GtWq/S8fr/pyFSaETsDk8MnQ09PDkX1HMPeTuZg2bxqat20O00emGDt2LAAgLCwMRUVF6NatGx4+fIgffvgBrq6uSE5Ohp6eHjw9PRETE4O5c+fiwoULSEpPgokpr4AjIqLKY9iRqJvXbkIIgTr16pQ5z/G/juPy+cvYemQr7BzsAAARX0egv09/nEs6h0bNGwF4evn9vK/nobp1dZXlu7zfBQEDApTPwz4JQ+CEQPTs1xMA0Mq+FT777DOEhIQgLCwMf/75J44fP47z58/D3d0dAFC3bl3l8paWlpDJZLCzs0PNopoa6QMRERHDjkQJ8fQbg190yXzKpRTY2tsqgw4A1HWvC3NLc1y7dE0Zdmo71C4RdADAo5mHyvPzf59H8ulkxH799BBZNVk1KBQK5Obm4smTJ0hKSsJbb72lDDpERESvA8OORDm6OEImkyHlUgq8u3qXOo8QAqV9BY0QQiUkGZmU/k3Izx8eE0Jg7NSx8OnmAwBoYtvkf+swMoKxccnDaURERK8aT1CWKMvqlnjH+x38EvcLcp7klJj+MOsh6rrXxb+3/kX6rXTl+NWLV/Eo+xHquNWp8GvWb1wfqVdS4ejiCEcXR9SrV0/5qFatGpo2bYqbN2/i4sWLpS5vaGgIhUJR4dclIiJ6EYYdCZsROQOKIgUCewRiz+97cP3qdaRcSsHG7zZiZMBItOnQBvU86mHux3Pxz5l/cO7UOYRPCsfb7d5Gw2YNK/x6oyePxu+//I5VX67ClQtXcP78efz000+YPXs2AMDLywsdO3bEBx98gPj4eKSkpGDHjh3YuXMnAKBOnTp49OgRdu/ejQf3HyA3J1ej/SAiojcTw46EOTg54IedP6CVZyvEzIvBgE4DMHHARCQcTMCnUZ9CJpNh4ZqFMLc0x9g+YzFhwAQ4ODkgcnmkWq/XzrsdFq9djGMHjiGweyDeeecdLFq0CM7Ozsp5fv31V7Ru3RoDBw5Ew4YNERISotyb4+npifHjx6N///7wbeKL75d9r5E+EBHRm00mis9kfYNlZ2fD0tISWVlZsLBQvWlkbm4uUlJS4OLiotZdvE+kndBUmVVOK/tWai+rkb4VAndv3cX4Q+OR+ji18ut7DURY5X4cX/eXXeoK9k09lenbm9ozgH1TR2V/Rsvyor/fz+KeHSIiIpI0hh0iIiKSNIYdIiIikjSGHSIiIpI0hh0iIiKSNIYdIiIikjSGHSIiIpI0hh0iIiKSNIYdIiIikjSGHSIiIpI0hh0JCw8Ox7SR07Rdxisxru84fDn3S22XQUREVYC+tguosmTlu7+J+neHUnXiVoKG1vT65Ofnw9DQUGVMoVBAJpOhWjXmbCIiej34F+cNMa7vOCycsxBff/41OjXqhC7Nu2DVl6tU5nmY9RDzQ+ajS7MuaF+3Pfq/1x9/xf+lnL7n9z3o59MPni6eCGgbgB9W/KCyfEDbAHwX8x3Cg8Ph3cAbY8aMQVxcHKysrLB9+3Y0bNgQcrkcqampyM/PR0hICBwcHGBqaoq2bdti3759Kus7nXAaYz8Yi3dd38V7Dd/Dx4M+RvaDbIQHhyPxSCI2frcRrR1ao7VDa6TdSHtlvSMioqqNe3beINs3bcfgsYMR+1sszpw8g4jJEWjWuhnadmyLoqIiTBoyCY8fP8a8JfPg4OyAlIspqKb3NA+f//s8QseHYsyUMfAN8MXfJ/7GgpkLYFndEv79/ZWvsW7FOowKHoVRk0ahiW0THDx4EE+ePEFUVBS+/fZbWFtbw8bGBiNGjMC1a9ewceNG2NvbY8uWLejatSvOnDkDNzc3XDh7AUH9g+Df3x/T5k2Dnr4eTh4+iaKiIkybNw3Xr16HawNXjJs2DgBQ3bq6VnpKRES6j2HnDeLm4YYxU8YAAJzqOuHnuJ9x/OBxtO3YFsf/Oo5zSefw876f4ezqDAB4y/kt5bLrV61H63dbY/Tk0QAAZ1dnpFxKwboV61TCTuv2rTF0/FAAQD37ejh48CAKCgqwbNkyNGvWDABw5coV/Pjjj7h58ybs7e0BANOmTcPOnTsRGxuLyMhIrFu+Dh5NPfBp1KfKdbvWd1X+v4GhAYyMjFDTpuaraBUREUkIw84bpJ5HPZXnNW1qIvNuJgDg4rmLsKltoww6z7t26Rq8unipjDVr3Qw/fvsjFAoF9PT0AAAeTT1KLGtoaIimTZsqnycmJkIIAXd3d5X58vLyYG1traynU89OFdxCIiKikhh23iD6+qpvt0wmQ1FREQBAbiR/4bJCCEBWythzjEyMSowZGxtD9swJ3UVFRdDT08PJkyeVIamYmZlZueohIiIqL56gTACe7vXJuJ2B1CuppU53cXfB6eOnVcb+PvE3nOo6lQgsL9OiRQsoFApkZGSgXr16Kg87OztlPQkHy74CzcDAQBnUiIiIXoRhhwAALdu1RIu2LTBj7AwcO3AMt67fwqE9h3B472EAwJBxQ5BwMAHfLv4WqVdSsf3n7fg59mcMGTekwq/l7u6OwYMHY9iwYdi8eTNSUlKQkJCABQsW4L///S8AYPjE4Ug+nYz/hP4Hl5Iv4drla/hl7S94cP8BAKC2Y22cPXUWaTfS8OD+AwYfIiIqE8MOKS1YvQANmzXErKBZ6O/TH0vmL0GR4mmIaNCkAaJWRGHXtl0Y0GkAVi5ciXHTx6mcnFwRsbGxGDZsGKZOnYr69esjICAAx44dg6OjI4CnJ0Av2bAEl5IvYXjP4RgZMBL7d+1X7kUaMm4I9KrpoZ93P/g28UX6rXTNNIGIiCRHJko78eINk52dDUtLS2RlZcHCwkJlWm5uLlJSUuDi4gIjo5Lno7zMibQTmiqzymllr/5XKmqkb4XA3Vt3Mf7QeKQ+Lv3wnK4RYZX7cZRFlO/LLqWGfVNPZfr2pvYMYN/UUdmf0bK86O/3s7hnh4iIiCSNYYeIiIgkjWGHiIiIJI1hh4iIiCSNYYeIiIgkjWGnnHjRWhUkAPH//yMiojcXw85LGBgYAACePHmi5UqowoqAwqJCZBdka7sSIiLSIt4b6yX09PRgZWWFjIwMAICJiYnKfZ5eqvAVFVYF5Obmqr9wZfsmgNysXCTcSUBWflYlV0ZERFUZw045FN+vqTjwVMTdB3c1XU6VkfI4Re1lK9s3AYG7OXex4sIKHsYiInrDMeyUg0wmQ+3atWFjY4OCgoIKLdttabdXVJXu+2fiP2ovW9m+KYQC6TnpKBRv8K41IiICIKGws2zZMnzxxRe4ffs2GjVqhJiYGHTo0EGjr6Gnp1fhO3xXldsUvArq3F6j2JvcNyIi0ixJnKD8008/ITg4GLNmzcKpU6fQoUMHdOvWDdevX9d2aURERKRlkgg7ixYtwqhRozB69Gh4eHggJiYGjo6OWL58ubZLIyIiIi2r8mEnPz8fJ0+ehJ+fn8q4n58fDh8+rKWqiIiISFdU+XN27t69C4VCAVtbW5VxW1tbpKenl7pMXl4e8vLylM+zsp5empyd/Qq+j6USV19XdZXq5xvat0p/Btk39bBvFfeG9gxg39TxSv6+PrPel33xb5UPO8We/+4bIUSZ34cTFRWFiIiIEuOOjo6vpLY3leV/LLVdQpXDnqmHfVMP+6Ye9q3iXnXPHj58CEvLsl+jyoedmjVrQk9Pr8RenIyMjBJ7e4qFhoZiypQpyudFRUW4f/8+rK2tK/aFgTouOzsbjo6OuHHjBiwsLLRdTpXAnqmHfVMP+6Ye9q3ipNozIQQePnwIe3v7F85X5cOOoaEhWrZsifj4ePTu3Vs5Hh8fj169epW6jFwuh1wuVxmzsrJ6lWVqlYWFhaQ+3K8De6Ye9k097Jt62LeKk2LPXrRHp1iVDzsAMGXKFAwdOhStWrVCu3btsGrVKly/fh3jx4/XdmlERESkZZIIO/3798e9e/cwb9483L59G40bN8Z///tfODs7a7s0IiIi0jJJhB0ACAoKQlBQkLbL0ClyuRxhYWElDtlR2dgz9bBv6mHf1MO+Vdyb3jOZeNn1WkRERERVWJX/UkEiIiKiF2HYISIiIklj2CEiIiJJY9ipImQyGbZu3artMqoU9kw97Jt62Df1sG8Vx55VHMOODkhPT8fHH3+MunXrQi6Xw9HREf7+/ti9e7e2SwPw9Bsqw8PDYW9vD2NjY3h7e+PcuXNarUnXe7Z582Z06dIFNWvWhEwmQ1JSkrZLAqDbfSsoKMCMGTPQpEkTmJqawt7eHsOGDUNaWpq2S9PpvgFAeHg4GjRoAFNTU1SvXh2dO3fGsWPHtF2WzvftWePGjYNMJkNMTIxW69D1ng0fPhwymUzl8c4772i7rJeSzKXnVdW1a9fQvn17WFlZITo6Gk2bNkVBQQH++OMPTJgwAf/884+2S0R0dDQWLVqEuLg4uLu74/PPP4evry8uXLgAc3Pz115PVejZ48eP0b59e3z44YcYM2aMtssBoPt9e/LkCRITEzFnzhw0a9YMmZmZCA4ORkBAAE6cOKG1unS9bwDg7u6OpUuXom7dusjJycHixYvh5+eHy5cvo1atWlqpqSr0rdjWrVtx7Nixl95y4FWrKj3r2rUrYmNjlc8NDQ21WE05CdKqbt26CQcHB/Ho0aMS0zIzM5X/D0Bs2bJF+TwkJES4ubkJY2Nj4eLiImbPni3y8/OV05OSkoS3t7cwMzMT5ubm4u233xYJCQlCCCGuXbsmevbsKaysrISJiYlo2LCh+P3330utr6ioSNjZ2Yn//Oc/yrHc3FxhaWkpVqxYUcmtV4+u9+xZKSkpAoA4deqU2turKVWpb8WOHz8uAIjU1NSKb7CGVMW+ZWVlCQDizz//rPgGa0hV6dvNmzeFg4ODOHv2rHB2dhaLFy+u1HZXRlXoWWBgoOjVq1elt/V1454dLbp//z527tyJ+fPnw9TUtMT0F92vy9zcHHFxcbC3t8eZM2cwZswYmJubIyQkBAAwePBgtGjRAsuXL4eenh6SkpJgYGAAAJgwYQLy8/Nx4MABmJqaIjk5GWZmZqW+TkpKCtLT0+Hn56cck8vl8PLywuHDhzFu3LhKdKDiqkLPdFFV7VtWVhZkMpnW7l1XFfuWn5+PVatWwdLSEs2aNav4RmtAVelbUVERhg4diunTp6NRo0aV2+hKqio9A4B9+/bBxsYGVlZW8PLywvz582FjY6P+xr8O2k5bb7Jjx44JAGLz5s0vnRfPJfnnRUdHi5YtWyqfm5ubi7i4uFLnbdKkiQgPDy9XjYcOHRIAxK1bt1TGx4wZI/z8/Mq1Dk2qCj17lq7s2alqfRNCiJycHNGyZUsxePBgtZbXhKrUt99++02YmpoKmUwm7O3txfHjxyu0vCZVlb5FRkYKX19fUVRUJIQQWt2zU1V6tnHjRrF9+3Zx5swZsW3bNtGsWTPRqFEjkZubW+51aAPDjhYdPXr0pR/aYs/Pt2nTJtG+fXtha2srTE1NhVwuF7Vq1VJODwsLE/r6+qJTp04iKipKXL58WTlt9erVQl9fX3h6eoq5c+eK06dPl/m6xWEnLS1NZXz06NGiS5cu5d9YDakKPXuWroSdqta3/Px80atXL9GiRQuRlZVV7u3UtKrUt0ePHolLly6JI0eOiJEjR4o6deqIf//9t0LbqylVoW8nTpwQtra2Kv+Q02bYqQo9K01aWpowMDAQv/76a4WWe90YdrTo3r17QiaTicjIyJfO++yH+8iRI0JPT098/vnnIiEhQVy8eFHMmzdPWFpaqixz4cIFsWjRIuHr6ysMDQ1V/sVw/fp1sXz5ctG7d29hYGAgvv7661Jf98qVKwKASExMVBkPCAgQw4YNq9gGa0BV6NmzdCXsVKW+5efni/fff180bdpU3L17t8LbqklVqW/Pq1evXrnqfhWqQt8WL14sZDKZ0NPTUz4AiGrVqglnZ2d1N11tVaFnZalXr57KeZ26iGFHy7p27VrhE9IWLlwo6tatqzLvqFGjSny4nzVgwADh7+9f6rRPP/1UNGnSpNRpxScoL1iwQDmWl5en1ROUdb1nz9KVsCNE1ehbcdBp1KiRyMjIKHtjXqOq0LfSuLq6irCwsAoto0m63re7d++KM2fOqDzs7e3FjBkzxD///PPijXtFdL1npbl7966Qy+Vi7dq15V5GG/g9O1q2bNkyKBQKtGnTBr/++isuXbqE8+fP4+uvv0a7du1KXaZevXq4fv06Nm7ciCtXruDrr7/Gli1blNNzcnIwceJE7Nu3D6mpqTh06BASEhLg4eEBAAgODsYff/yBlJQUJCYmYs+ePcppz5PJZAgODkZkZCS2bNmCs2fPYvjw4TAxMcGgQYM035By0PWeAU9PNkxKSkJycjIA4MKFC0hKSkJ6eroGO1Exut63wsJC9O3bFydOnMD69euhUCiQnp6O9PR05Ofna74h5aTrfXv8+DFmzpyJo0ePIjU1FYmJiRg9ejRu3ryJDz/8UPMNKSdd75u1tTUaN26s8jAwMICdnR3q16+v+YaUg6737NGjR5g2bRqOHDmCa9euYd++ffD390fNmjXRu3dvzTdEk7SdtujpMc8JEyYIZ2dnYWhoKBwcHERAQIDYu3evch48d4x2+vTpwtraWpiZmYn+/fuLxYsXK5N8Xl6eGDBggHB0dBSGhobC3t5eTJw4UeTk5AghhJg4caJwdXVVHtcdOnToCw8XFBUVibCwMGFnZyfkcrno2LGjOHPmzKtoRbnpes9iY2MFgBIPbf5LWwjd7lvxXrDSHs/Wpw263LecnBzRu3dvYW9vLwwNDUXt2rVFQECAVk9QLqbLfSuNti89F0K3e/bkyRPh5+cnatWqJQwMDISTk5MIDAwU169ff1Xt0BiZEEK89oRFRERE9JrwMBYRERFJGsMOERERSRrDDhEREUkaww4RERFJGsMOERERSRrDDhEREUkaww4RERFJGsMOEb1yMpkMW7du1XYZagkPD0fz5s0rtY5r165BJpMhKSlJIzURUcUw7BBRpaSnp+Pjjz9G3bp1IZfL4ejoCH9/f+zevVvbpQEAvL29ERwcrO0yiEiL9LVdABFVXdeuXUP79u1hZWWF6OhoNG3aFAUFBfjjjz8wYcIE/PPPP9oukYiIe3aISH1BQUGQyWQ4fvw4+vbtC3d3dzRq1AhTpkzB0aNHy1xuxowZcHd3h4mJCerWrYs5c+agoKBAOf306dPw8fGBubk5LCws0LJlS5w4cQIAkJqaCn9/f1SvXh2mpqZo1KgR/vvf/6q9DS+rpdjKlSvh6OgIExMTfPjhh3jw4IHK9NjYWHh4eMDIyAgNGjTAsmXLynzNzMxMDB48GLVq1YKxsTHc3NwQGxur9jYQ0Ytxzw4RqeX+/fvYuXMn5s+fD1NT0xLTraysylzW3NwccXFxsLe3x5kzZzBmzBiYm5sjJCQEADB48GC0aNECy5cvh56eHpKSkmBgYAAAmDBhAvLz83HgwAGYmpoiOTkZZmZmam/Hy2oBgMuXL+Pnn3/Gb7/9huzsbIwaNQoTJkzA+vXrAQCrV69GWFgYli5dihYtWuDUqVMYM2YMTE1NERgYWOI158yZg+TkZOzYsQM1a9bE5cuXkZOTo/Y2ENFLaPtOpERUNR07dkwAEJs3b37pvHjuLs3Pi46OFi1btlQ+Nzc3F3FxcaXO26RJExEeHl7uOr28vMSkSZPKPf/ztYSFhQk9PT1x48YN5diOHTtEtWrVxO3bt4UQQjg6OooNGzaorOezzz4T7dq1E0L8747up06dEkII4e/vL0aMGFHumoiocrhnh4jUIoQA8PRKq4r65ZdfEBMTg8uXL+PRo0coLCyEhYWFcvqUKVMwevRorFu3Dp07d8aHH34IV1dXAMAnn3yCjz76CLt27ULnzp3xwQcfoGnTpmpvx8tqAQAnJye89dZbyuft2rVDUVERLly4AD09Pdy4cQOjRo3CmDFjlPMUFhbC0tKy1Nf86KOP8MEHHyAxMRF+fn54//334enpqfY2ENGL8ZwdIlKLm5sbZDIZzp8/X6Hljh49igEDBqBbt27Yvn07Tp06hVmzZiE/P185T3h4OM6dO4cePXpgz549aNiwIbZs2QIAGD16NK5evYqhQ4fizJkzaNWqFZYsWaLWNpSnltIUBzyZTIaioiIATw9lJSUlKR9nz54t87ylbt26ITU1FcHBwUhLS0OnTp0wbdo0tbaBiF6OYYeI1FKjRg106dIF33zzDR4/flxi+vMn8BY7dOgQnJ2dMWvWLLRq1Qpubm5ITU0tMZ+7uzsmT56MXbt2oU+fPion8Do6OmL8+PHYvHkzpk6ditWrV6u1DeWt5fr160hLS1M+P3LkCKpVqwZ3d3fY2trCwcEBV69eRb169VQeLi4uZb52rVq1MHz4cPzwww+IiYnBqlWr1NoGIno5HsYiIrUtW7YMnp6eaNOmDebNm4emTZuisLAQ8fHxWL58eal7ferVq4fr169j48aNaN26NX7//XflXhsAyMnJwfTp09G3b1+4uLjg5s2bSEhIwAcffAAACA4ORrdu3eDu7o7MzEzs2bMHHh4eL6zzzp07Jb7Qz87O7qW1FDMyMkJgYCAWLlyI7OxsfPLJJ+jXrx/s7OwAPN0T9cknn8DCwgLdunVDXl4eTpw4gczMTEyZMqXE+ubOnYuWLVuiUaNGyMvLw/bt21+6DURUCdo+aYiIqra0tDQxYcIE4ezsLAwNDYWDg4MICAgQe/fuVc6D505Qnj59urC2thZmZmaif//+YvHixcLS0lIIIUReXp4YMGCAcHR0FIaGhsLe3l5MnDhR5OTkCCGEmDhxonB1dRVyuVzUqlVLDB06VNy9e7fM+ry8vASAEo+wsLCX1iLE0xOUmzVrJpYtWybs7e2FkZGR6NOnj7h//77K66xfv140b95cGBoaiurVq4uOHTsqT95+/gTlzz77THh4eAhjY2NRo0YN0atXL3H16lX13gAieimZEP//LEMiIiIiCeI5O0RERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaQw7REREJGkMO0RERCRpDDtEREQkaQw7REREJGn/D31+9ZmSiB6KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = np.argmax(test_predictions.predictions, axis=1)  \n",
    "true_labels = test_predictions.label_ids \n",
    "\n",
    "num_classes = len(np.unique(true_labels))\n",
    "correct_counts = np.zeros(num_classes)\n",
    "incorrect_counts = np.zeros(num_classes)\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "    if predictions[i] == true_labels[i]:\n",
    "        correct_counts[true_labels[i]] += 1\n",
    "    else:\n",
    "        incorrect_counts[true_labels[i]] += 1\n",
    "\n",
    "total_counts = correct_counts + incorrect_counts\n",
    "correct_percentages = correct_counts / total_counts * 100\n",
    "incorrect_percentages = incorrect_counts / total_counts * 100\n",
    "\n",
    "labels = [f\"Class {i}\" for i in range(num_classes)]\n",
    "x = np.arange(num_classes)\n",
    "\n",
    "plt.bar(x, correct_percentages, label=\"Correct\", color=\"green\")\n",
    "plt.bar(x, incorrect_percentages, bottom=correct_percentages, label=\"Incorrect\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Correct vs Incorrect Predictions per Class\")\n",
    "plt.xticks(x, labels)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbea8f-f026-45f9-9bce-ff1fc91e6937",
   "metadata": {
    "id": "61bbea8f-f026-45f9-9bce-ff1fc91e6937"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "Compared with the previous F1 score (about 0.927), there is a slight improvement. This shows that after dealing with the class imbalance problem through the weighted loss function, the overall performance of the model on the test set has improved.\n",
    "\n",
    "Most categories (such as Class 0, Class 1, Class 2, and Class 3) maintain a high correct classification ratio (the green part is close to 100%).\n",
    "The misclassification ratio of Class 5 (the red part) has decreased. Compared with the previous bar chart, the model's recognition ability for minority categories has improved.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37211a4f-1007-43d2-8d64-e6b5e29b513f",
   "metadata": {
    "id": "37211a4f-1007-43d2-8d64-e6b5e29b513f"
   },
   "source": [
    "# Task 2: Text Summarization with T5\n",
    "In this task, we will use the [SamSum](https://huggingface.co/datasets/samsum) dataset, which is a collection of back-and-forth chats between people, and a narrator-style summary of the chat.\n",
    "We will fine-tune our T5 model on this dataset and experiment with it on our own invented chats to see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b87c73-32c0-441a-910c-91129b5779e0",
   "metadata": {
    "id": "e8b87c73-32c0-441a-910c-91129b5779e0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a254090d-56b9-474c-b3f1-011c8b707692",
   "metadata": {
    "id": "a254090d-56b9-474c-b3f1-011c8b707692"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57cc35-bc28-4881-aec1-1e3e02df9b19",
   "metadata": {
    "id": "0e57cc35-bc28-4881-aec1-1e3e02df9b19"
   },
   "source": [
    "* Download the dataset from the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575512e5-9b1a-4250-9c61-c6dbc24b5848",
   "metadata": {
    "id": "575512e5-9b1a-4250-9c61-c6dbc24b5848"
   },
   "outputs": [],
   "source": [
    "samsum = load_dataset(\"samsum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9f4e7-80ff-42ac-9526-884e69f9875c",
   "metadata": {
    "id": "26d9f4e7-80ff-42ac-9526-884e69f9875c"
   },
   "source": [
    "* Use Hugging Face's `pipeline` interface to see the non-finetuned performance of our model.\n",
    "* Load the [T5-small](https://huggingface.co/t5-small) model inside the [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines) object, along with the `task=summarization` attribute.\n",
    "* Pick a sample from the [SamSum Test Set](https://huggingface.co/datasets/samsum/viewer/samsum/test). You can directly copy it from the website if you like.\n",
    "* Input it into the created pipeline object and check the result.\n",
    "    * In case of memory problems later on during fine-tuning, `del` the pipeline object to free up memory afterwards.\n",
    "* Comment: How well did it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1db252a-ad67-4397-a693-554493564d6f",
   "metadata": {
    "id": "b1db252a-ad67-4397-a693-554493564d6f"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4eba82f-66b4-4d28-900b-348a63a944d1",
   "metadata": {
    "id": "c4eba82f-66b4-4d28-900b-348a63a944d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 133. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue:\n",
      " Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Generated Summary:\n",
      " Amanda: Lemme check Hannah: file_gif> Amanda: Sorry, can't find it . he called her last time we were at the park together Hannah: I don't know him well .\n"
     ]
    }
   ],
   "source": [
    "sample_dialogue = samsum[\"test\"][0][\"dialogue\"]\n",
    "print(\"Original Dialogue:\\n\", sample_dialogue)\n",
    "summary = pipe(sample_dialogue)\n",
    "print(\"\\nGenerated Summary:\\n\", summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b258c2-9ecc-4c24-9bc6-8479fffad285",
   "metadata": {
    "id": "d6b258c2-9ecc-4c24-9bc6-8479fffad285"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "Instead of successfully generating a concise summary, the model simply captured parts of the original conversation.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608e2f0-dde3-4f7b-95f4-cdbaad81f1be",
   "metadata": {
    "id": "1608e2f0-dde3-4f7b-95f4-cdbaad81f1be"
   },
   "source": [
    "* Then, we will load [T5's](https://huggingface.co/t5-small) `t5-small` tokenizer, again using `AutoTokenizer` from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701bfc05-bc7d-405e-afec-6b09b9e42c86",
   "metadata": {
    "id": "701bfc05-bc7d-405e-afec-6b09b9e42c86"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ab0c8-4388-4163-95f1-102a8fef3085",
   "metadata": {
    "id": "4c5ab0c8-4388-4163-95f1-102a8fef3085"
   },
   "source": [
    "T5 is a sequence-to-sequence encoder-decoder model, meaning that it takes sequences as input, transforms them, and outputs target sequences. The model can be used for translation, classification, summarization, and much more, given the correct input prompts.\n",
    "\n",
    "As we are dealing with a summarization task, we need to preface our sequences with the necessary `'summarize: '` prompt.\n",
    "* Write a preprocess function which takes in a dataset split and prefaces every input sequence with the string `'summarize: '`\n",
    "* Afterwards, use the tokenizer with the arguments `truncation=True` and `max_length=512` to limit the maximum lengths of all inputs, and tokenize all sequences\n",
    "* Then, separately transform the target sequences using `truncation=True` with a `max_length=128`\n",
    "* Extract the `input_ids` from the transformed target sequences and add them to the the dictionary output of the transformed input sequences with the key `labels`\n",
    "* Return the dictionary object afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d7d0c26-fe49-4224-93a6-40d484c500fc",
   "metadata": {
    "id": "1d7d0c26-fe49-4224-93a6-40d484c500fc"
   },
   "outputs": [],
   "source": [
    "def preprocess_summarization(split):\n",
    "    \n",
    "    inputs = [\"summarize: \" + dialogue for dialogue in split[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(split[\"summary\"], max_length=128, truncation=True, padding=\"max_length\") \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727e745-bfab-4612-aa30-3b78e6defa2e",
   "metadata": {
    "id": "0727e745-bfab-4612-aa30-3b78e6defa2e"
   },
   "source": [
    "* As before with the classification task, `map` the preprocessing function onto the entire dataset using the `batched=True` option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310ee61b-5b80-43a0-8a5d-d76c69dddff2",
   "metadata": {
    "id": "310ee61b-5b80-43a0-8a5d-d76c69dddff2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c8d91416c34f03bfbe78073bd0f5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized_samsum = samsum.map(preprocess_summarization, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b20fa-e22d-42d1-b40f-4414b9f2a819",
   "metadata": {
    "id": "aa2b20fa-e22d-42d1-b40f-4414b9f2a819"
   },
   "source": [
    "* As the next step, we create a `collator` [object](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq), which takes in the `tokenizer` and our `model`\n",
    "    * At this stage, it is fine to just write `model='t5-small'` as input to the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611afa55-9d71-4a5f-8017-3abce46ccd51",
   "metadata": {
    "id": "611afa55-9d71-4a5f-8017-3abce46ccd51"
   },
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119c26e-7549-4995-a05d-1374c1e7b85c",
   "metadata": {
    "id": "2119c26e-7549-4995-a05d-1374c1e7b85c"
   },
   "source": [
    "* To evaluate our model's summarization performance, we employ the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) score.\n",
    "* Advantages include:\n",
    "    * The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric commonly used for evaluating the quality of machine-generated text, particularly summaries\n",
    "    * ROUGE measures the similarity between the generated summary and one or more reference (human-written) summaries\n",
    "    * ROUGE provides a standardized way to evaluate the quality of text summaries, which is important for at-scale comparison between different summarization models\n",
    "    * ROUGE includes multiple metrics, such as ROUGE-N (for n-grams), ROUGE-L (for longest common subsequence), and ROUGE-W (for weighted n-grams). Depending on the task, these metrics capture different aspects of summary quality, allowing a more comprehensive evaluation.\n",
    "* Disadvantages include:\n",
    "    * ROUGE relies on exact word or n-gram matches, which may not capture the semantic meaning well. Two sentences with similar meaning but slightly different phrasing might receive a low ROUGE score. (You can use the above link to play around with some references and generated sequences)\n",
    "    * ROUGE does not consider the order of words or phrases. If a model generates a summary with words or phrases in a different order than the reference, even if the content is correct, the ROUGE score might be lower.\n",
    "    * ROUGE measures lexical overlap and does not consider deeper linguistic structures or coherence in summaries.\n",
    "* Load the `rouge` score from the `evaluate` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd578c4e-47c9-4d6d-bd4d-91a509cf8129",
   "metadata": {
    "id": "cd578c4e-47c9-4d6d-bd4d-91a509cf8129"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba798853-94c9-4013-9fbb-514e1d166dd7",
   "metadata": {
    "id": "ba798853-94c9-4013-9fbb-514e1d166dd7"
   },
   "source": [
    "* Then, we need to define our `compute_metrics` function, but this time for the ROUGE score\n",
    "* At first, we receive again our tuple of `EvalPred` predictions and labels, which is also the input to our function\n",
    "    * Split up the input tuple by extracting the labels and predictions\n",
    "    * In the T5 case, the predictions are again a tuple of `(logits, hidden_representations)`\n",
    "    * For our task, extract only the logits\n",
    "    * Apply `argmax()` on the last dimension of the logits to get the predictions\n",
    "* Then, we use our `tokenizer` to `batch_decode` the argmaxed logits\n",
    "    * Here, we activate the tokenizer setting `skip_special_tokens=True`, and `clean_up_tokenization_spaces=True`\n",
    "* Before we can decode our labels, we must first replace the padding tokens of the labels batch, represented by the value `-100`, with the actual padding tokens of the tokenizer\n",
    "    * The `-100` are a convention of the `Seq2SeqTrainer`, and don't have any deeper meaning\n",
    "    * the padding token can be accessed by `tokenizer.pad_token_id`\n",
    "* Then, we also `batch_decode` the labels\n",
    "    * Also skip the special tokens, but no need clean up tokenization spaces since we handled them separately\n",
    "* Afterwards, we input both decoded predictions and labels into the `rouge.compute` function\n",
    "* That result dictionary is returned\n",
    "    * Optionally, round all numbers in the result dictionary to 4 or 6 digits, whichever format you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df12172f-7fc2-437d-b167-b843f694d3b1",
   "metadata": {
    "id": "df12172f-7fc2-437d-b167-b843f694d3b1"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {key: round(value, 4) for key, value in result.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dff509-090d-4f01-a5dc-b02ecacc7069",
   "metadata": {
    "id": "41dff509-090d-4f01-a5dc-b02ecacc7069"
   },
   "source": [
    "* Then we create a `Seq2SeqTrainingArguments` [instance](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments), which is just a specialized version of our general `TrainingArguments` function from before.\n",
    "* In our case, we can base our settings on the training arguments from BERT classification, except:\n",
    "    * Since we now have longer sequences, it might be necessary to reduce batch sizes even more, so experiment a bit with your GPU memory availability\n",
    "        * Try to max-out your GPU memory by increasing the batch size as much as possible to speed up training\n",
    "    * change the `output_dir` to something under the same path from before, e.g. `'./logs/run3/'`\n",
    "* Once created, load the model using the `AutoModelForSeq2SeqLM`\n",
    "* Instantiate a `Seq2SeqTrainer` [object](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.Seq2SeqTrainer) in the same style as before\n",
    "    * This time, we just need to add the `collator` object to the `data_collator` argument\n",
    "* Call the `train()` method and start training\n",
    "* Comment briefly on the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "384c2efe-6aa7-4ad6-9705-bf4dffef05cf",
   "metadata": {
    "id": "384c2efe-6aa7-4ad6-9705-bf4dffef05cf"
   },
   "outputs": [],
   "source": [
    "training_args3 = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./logs/run3/\",  \n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,  \n",
    "    predict_with_generate=True,  \n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",  \n",
    "    logging_dir=\"./logs/run3/logs\",  \n",
    "    logging_strategy=\"steps\",  \n",
    "    logging_steps=500,  \n",
    "    save_total_limit=2,  \n",
    "    num_train_epochs=3,  \n",
    "    fp16=True,  \n",
    "    dataloader_num_workers=2,  \n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"rouge1\",  \n",
    "    greater_is_better=True,  \n",
    "    seed=42,  \n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  \n",
    "    args=training_args3,  \n",
    "    train_dataset=tokenized_samsum[\"train\"],  \n",
    "    eval_dataset=tokenized_samsum[\"validation\"],  \n",
    "    tokenizer=tokenizer,  \n",
    "    data_collator=collator,  \n",
    "    compute_metrics=compute_metrics,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc7c72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11049\n",
      "  Number of trainable parameters = 60506624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11049' max='11049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11049/11049 7:23:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>0.394717</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>0.183900</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.385695</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.384190</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.355800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./logs/run3/checkpoint-3683\n",
      "Configuration saved in ./logs/run3/checkpoint-3683\\config.json\n",
      "Model weights saved in ./logs/run3/checkpoint-3683\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run3/checkpoint-3683\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run3/checkpoint-3683\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./logs/run3/checkpoint-7366\n",
      "Configuration saved in ./logs/run3/checkpoint-7366\\config.json\n",
      "Model weights saved in ./logs/run3/checkpoint-7366\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run3/checkpoint-7366\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run3/checkpoint-7366\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./logs/run3/checkpoint-11049\n",
      "Configuration saved in ./logs/run3/checkpoint-11049\\config.json\n",
      "Model weights saved in ./logs/run3/checkpoint-11049\\pytorch_model.bin\n",
      "tokenizer config file saved in ./logs/run3/checkpoint-11049\\tokenizer_config.json\n",
      "Special tokens file saved in ./logs/run3/checkpoint-11049\\special_tokens_map.json\n",
      "Deleting older checkpoint [logs\\run3\\checkpoint-3683] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./logs/run3/checkpoint-11049 (score: 0.4195).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:2024: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: dialogue, summary, id. If dialogue, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 818\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [205/205 05:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.38418951630592346, 'eval_rouge1': 0.4195, 'eval_rouge2': 0.1949, 'eval_rougeL': 0.3561, 'eval_rougeLsum': 0.3558, 'eval_runtime': 336.2857, 'eval_samples_per_second': 2.432, 'eval_steps_per_second': 0.61, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d3881-7ffc-493f-ba2e-0594560e3e98",
   "metadata": {
    "id": "248d3881-7ffc-493f-ba2e-0594560e3e98"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "Loss curve:\n",
    "\n",
    "Training Loss and Validation Loss are gradually decreasing, indicating that the model is learning stably.\n",
    "The final Training Loss and Validation Loss are 0.4217 and 0.3842 respectively.\n",
    "Rouge indicators:\n",
    "\n",
    "Rouge1, Rouge2, RougeL and Rougesum indicators are gradually increasing, indicating that the quality of the summary generated by the model is gradually improving.\n",
    "In the end, Rouge1 = 0.4195, Rouge2 = 0.1949, RougeL = 0.3561, and the overall performance is good.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1afc9-c18a-4eef-9026-b569dff590ec",
   "metadata": {
    "id": "3ad1afc9-c18a-4eef-9026-b569dff590ec"
   },
   "source": [
    "* As the last step of this exercise, move the `model`, which represents the already loaded best model due to our `Seq2SeqTrainingArguments` settings, to the CPU and input it into a new `pipeline` object as the `model=` keyword.\n",
    "* Additionally, prove the `tokenizer=` argument\n",
    "* Then, fill in your chosen test set sample from before\n",
    "* Compare the results to your initial, pre fine-tuning output and discuss what got better or worse\n",
    "* Comment on whether the narrator-style summary performance increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74f6176-f248-4582-83bb-16c440774ce1",
   "metadata": {
    "id": "a74f6176-f248-4582-83bb-16c440774ce1"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=model.to(\"cpu\"), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f048bdb0-42f9-4586-8a30-e49bd48a8f22",
   "metadata": {
    "id": "f048bdb0-42f9-4586-8a30-e49bd48a8f22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 133. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue:\n",
      " Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Generated Summary:\n",
      " Larry called Betty last time she was at the park together. He called her last time they were there together. Hannah would rather text Betty's number.\n"
     ]
    }
   ],
   "source": [
    "sample_dialogue = tokenized_samsum[\"test\"][0][\"dialogue\"]\n",
    "print(\"Original Dialogue:\\n\", sample_dialogue)\n",
    "\n",
    "summary = pipe(sample_dialogue)\n",
    "print(\"\\nGenerated Summary:\\n\", summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8ac38-1b16-4a42-aa24-e85c2a14a536",
   "metadata": {
    "id": "a5c8ac38-1b16-4a42-aa24-e85c2a14a536"
   },
   "source": [
    "___\n",
    "Student answers here:\n",
    "\n",
    "Before fine-tuning, the summaries generated by the model were very confusing and often simply copied snippets of conversations.\n",
    "After fine-tuning, the resulting summary is more logical and focuses on the key events of the conversation (Larry and Betty's connection).\n",
    "Overall, fine-tuning significantly improved the performance of the model.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e576fcf-04ba-497d-9bc7-734bcff18979",
   "metadata": {
    "id": "0e576fcf-04ba-497d-9bc7-734bcff18979"
   },
   "source": [
    "# Inspect and share all your experiment results via TensorBoard (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb91278-d706-4daa-9521-8baa50996218",
   "metadata": {
    "id": "acb91278-d706-4daa-9521-8baa50996218"
   },
   "source": [
    "* While we now only had 3 runtime variations, in practice we often have tens or hundreds of hyperparameter and/or model combinatinations\n",
    "* To assemble manual train/val/test plots, scale all x- and y-axes for various units, and plot all desired combinations side by side is very cumbersome and time consuming\n",
    "* Services like [tensorboard-dev](https://tensorboard.dev/) and [tensorboard in general](https://www.tensorflow.org/tensorboard) offer the possibility to simply read in our created `'logs'` directory, which enables us to browse through an online interface of automatically generated plots for all our tracked metrics\n",
    "* Install (if you don't already have it) `tensorboard` via `pip install -U tensorboard)` and either:\n",
    "    * Upload your `'logs'` directly to Google (requires a Google account and consent to store the specific `'logs'` directory on Google's servers)\n",
    "        * execute on your command line `tensorboard dev upload --logdir logs --name \"whatever-name-you-want-to-call-it\"`\n",
    "        * then you will need to login to your Google account, consent to uploading the `'logs'` directory, and a shareable link will appear as command line output\n",
    "        * the link will lead you to the hosted experiment plots\n",
    "        * FYI: it is also possible to delete uploaded tensorboards via `tensorboard dev delete --experiment_id EXPERIMENT_ID`\n",
    "    * If you don't want to upload your experiments to Google or don't have a Google account, you can achieve the same browsable layout on your local machine\n",
    "    * execute on your command line `tensorboard --logdir logs`\n",
    "    * a localhost instance, usually on `https://localhost:6006/`, will open\n",
    "* Choose one of the above options to visualize your experiments in tensorboard\n",
    "* Include a screenshot of the results in the notebook submission of this exercise\n",
    "    * You can embed images in Jupyerlab in markdown cells [as described here](https://stackoverflow.com/questions/41604263/how-do-i-display-local-image-in-markdown)\n",
    "    * Also send us the screenshot file with it, otherwise we can't see your embedded image in the notebook\n",
    "* Last note: You aren't dependend on Hugging Face experiments to upload and visualize results to tensorboard\n",
    "    * You can upload every folder that is generated in the same format to tensorboard\n",
    "    * For instance, the same can be achived using [PyTorch](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) and the SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f75ed-1263-474b-9a99-1eacc6cfc733",
   "metadata": {
    "id": "0e8f75ed-1263-474b-9a99-1eacc6cfc733"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
